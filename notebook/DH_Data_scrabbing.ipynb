{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991db548-8934-44fe-9608-3a581720de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "sys.path.append(\"./../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a78f401-60b1-40d0-b8a2-94b1da80845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JDR #examples : 473\n",
      "JDF #examples : 500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Les entités nommées</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>label</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Faustin</td>\n",
       "      <td>Human</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chabot</td>\n",
       "      <td>Human</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>Location</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rue</td>\n",
       "      <td>Location</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Descartes</td>\n",
       "      <td>Location</td>\n",
       "      <td>35</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        form     label  begin  end\n",
       "0    Faustin     Human      0    7\n",
       "1     Chabot     Human      8   14\n",
       "2         19  Location     28   30\n",
       "3        rue  Location     31   34\n",
       "4  Descartes  Location     35   44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Human', 'Location', 'Reference_Code_Postal', 'Reference_CEDEX', 'Reference_CS', ..., 'Phone_Number', 'Social_Network', 'Reference_User', 'Organization', 'Url']\n",
      "Length: 13\n",
      "Categories (13, object): ['Email', 'Function', 'Human', 'Location', ..., 'Reference_Code_Postal', 'Reference_User', 'Social_Network', 'Url']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "DATA_PATH = path.join('..','dataset')\n",
    "JDF_PATH = path.join(DATA_PATH, 'JDF.json')\n",
    "JDR_PATH = path.join(DATA_PATH, 'JDR.json')\n",
    "\n",
    "data = dict()\n",
    "\n",
    "with open(JDR_PATH, 'r') as f:\n",
    "    data['jdr'] = json.load(f)\n",
    "    \n",
    "with open(JDF_PATH, 'r') as f:\n",
    "    data['jdf'] = json.load(f)\n",
    "\n",
    "print('JDR #examples :',len(data['jdr']))\n",
    "print('JDF #examples :',len(data['jdf']))\n",
    "\n",
    "annotations = [a for d in data['jdr'] for a in d['annotations']]\n",
    "df_annotations = pd.DataFrame(annotations)\n",
    "df_annotations[\"label\"] = df_annotations[\"label\"].astype(\"category\")\n",
    "display(HTML('<h3>Les entités nommées</h3>'))\n",
    "display(df_annotations.head())\n",
    "\n",
    "print('Labels:', df_annotations['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab065a9-153a-41be-8c8b-8b1a68e4dff2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Format d'une pharse donnée</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a14ce8-46de-4929-97bc-94b7dcb2dfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'identifier': 500,\n",
       " 'text': 'Faustin Chabot\\\\r\\\\nAdresse : 19 rue Descartes 94370 Sucy-en-Brie (France)\\\\r\\\\nCedex 9 CS 12468\\\\r\\\\nData Engineer / Algorithm XZ Project\\\\r\\\\nfaustinchabot@teleworm.com / Tel : +33 0134354919\\\\r\\\\nLinkedin : https://fr.linkedin.com/in/fauchab\\\\r\\\\nTeleworm France\\\\r\\\\nteleworm.france.com',\n",
       " 'annotations': [{'form': 'Faustin', 'label': 'Human', 'begin': 0, 'end': 7},\n",
       "  {'form': 'Chabot', 'label': 'Human', 'begin': 8, 'end': 14},\n",
       "  {'form': '19', 'label': 'Location', 'begin': 28, 'end': 30},\n",
       "  {'form': 'rue', 'label': 'Location', 'begin': 31, 'end': 34},\n",
       "  {'form': 'Descartes', 'label': 'Location', 'begin': 35, 'end': 44},\n",
       "  {'form': '94370', 'label': 'Reference_Code_Postal', 'begin': 45, 'end': 50},\n",
       "  {'form': 'Sucy-en-Brie', 'label': 'Location', 'begin': 51, 'end': 63},\n",
       "  {'form': 'France', 'label': 'Location', 'begin': 65, 'end': 71},\n",
       "  {'form': 'Cedex', 'label': 'Reference_CEDEX', 'begin': 76, 'end': 81},\n",
       "  {'form': '9', 'label': 'Reference_CEDEX', 'begin': 82, 'end': 83},\n",
       "  {'form': 'CS', 'label': 'Reference_CS', 'begin': 84, 'end': 86},\n",
       "  {'form': '12468', 'label': 'Reference_CS', 'begin': 87, 'end': 92},\n",
       "  {'form': 'Data', 'label': 'Function', 'begin': 96, 'end': 100},\n",
       "  {'form': 'Engineer', 'label': 'Function', 'begin': 101, 'end': 109},\n",
       "  {'form': 'Algorithm', 'label': 'Project', 'begin': 112, 'end': 121},\n",
       "  {'form': 'XZ', 'label': 'Project', 'begin': 122, 'end': 124},\n",
       "  {'form': 'Project', 'label': 'Project', 'begin': 125, 'end': 132},\n",
       "  {'form': 'faustinchabot@teleworm.com',\n",
       "   'label': 'Email',\n",
       "   'begin': 136,\n",
       "   'end': 162},\n",
       "  {'form': '+33 0134354919',\n",
       "   'label': 'Phone_Number',\n",
       "   'begin': 171,\n",
       "   'end': 185},\n",
       "  {'form': 'Linkedin', 'label': 'Social_Network', 'begin': 189, 'end': 197},\n",
       "  {'form': 'https://fr.linkedin.com/in/fauchab',\n",
       "   'label': 'Reference_User',\n",
       "   'begin': 200,\n",
       "   'end': 234},\n",
       "  {'form': 'Teleworm', 'label': 'Organization', 'begin': 238, 'end': 246},\n",
       "  {'form': 'France', 'label': 'Organization', 'begin': 247, 'end': 253},\n",
       "  {'form': 'teleworm.france.com', 'label': 'Url', 'begin': 257, 'end': 276}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['jdr'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8240750-6ed2-4bf7-affa-d81db22eae02",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Est-ce qu'il y a de motif pour le numéro de téléphone?</div>\n",
    "\n",
    "> Apparemment non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538bdd22-5edd-488f-9e6d-57c720ddb879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>label</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>+33 0134354919</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>171</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>03.18.38.37.37</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>136</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>+ 03 81 20 48 27</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>152</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>01.75.88.25.30</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>77</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>+ 33 01 77 83 74 05</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>157</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>+33 0365962110</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>70</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>01.55.29.21.75</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>118</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>+ 33 01 79 28 30 87</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>75</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>33 0147908347</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>150</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>03.54.57.86.42</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>162</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>+ 33 0529308213</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>113</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>03 46 69 07 08</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>121</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>+ 33 0221446127</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>129</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>01.92.26.82.75</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>124</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>01.17.82.71.60</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>151</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>+33 01.26.31.62.25</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>86</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>02 66 69 43 14</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>143</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>02.96.95.55.61</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>146</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>+ 33 01.47.04.39.40</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>134</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>+33 04.12.26.56.34</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>90</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    form         label  begin  end\n",
       "18        +33 0134354919  Phone_Number    171  185\n",
       "45        03.18.38.37.37  Phone_Number    136  150\n",
       "72      + 03 81 20 48 27  Phone_Number    152  168\n",
       "91        01.75.88.25.30  Phone_Number     77   91\n",
       "122  + 33 01 77 83 74 05  Phone_Number    157  176\n",
       "134       +33 0365962110  Phone_Number     70   84\n",
       "170       01.55.29.21.75  Phone_Number    118  132\n",
       "191  + 33 01 79 28 30 87  Phone_Number     75   94\n",
       "225        33 0147908347  Phone_Number    150  163\n",
       "250       03.54.57.86.42  Phone_Number    162  176\n",
       "269      + 33 0529308213  Phone_Number    113  128\n",
       "291       03 46 69 07 08  Phone_Number    121  135\n",
       "315      + 33 0221446127  Phone_Number    129  144\n",
       "333       01.92.26.82.75  Phone_Number    124  138\n",
       "364       01.17.82.71.60  Phone_Number    151  165\n",
       "378   +33 01.26.31.62.25  Phone_Number     86  104\n",
       "418       02 66 69 43 14  Phone_Number    143  157\n",
       "443       02.96.95.55.61  Phone_Number    146  160\n",
       "465  + 33 01.47.04.39.40  Phone_Number    134  153\n",
       "482   +33 04.12.26.56.34  Phone_Number     90  108"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations[df_annotations['label']=='Phone_Number'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae549953-f48a-4da9-9b86-9c14f7c16f64",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Liste des étiquettes à prédire</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b3bec3-c292-4e58-a83e-63a58ab880c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human',\n",
       " 'Location',\n",
       " 'Reference_Code_Postal',\n",
       " 'Reference_CEDEX',\n",
       " 'Reference_CS',\n",
       " 'Function',\n",
       " 'Project',\n",
       " 'Email',\n",
       " 'Phone_Number',\n",
       " 'Social_Network',\n",
       " 'Reference_User',\n",
       " 'Organization',\n",
       " 'Url']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_annotations['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95b0a9-07e5-47b0-b7aa-2c92694c09ab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Les étiquettes sont-elles chevauchées?</div>\n",
    "\n",
    "> Oui, il semble un bruit dans l'outil d'annotation. Il suffit de surrprimer celui contenu dans la vraie étiquette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7e5f47-c02e-494a-8949-515b7f5c9e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>jdr</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: Sibyla Chandonnet\\nTechnicienne en radiologie / Health For All\\nTel : +33 0365962110 \\nsibylachandonnet@yahoo.fr\\n89 rue du Général Ailleret 62300 Lens\\nCedex 08 CS 40362\\nThe Happy Bear / happybearhealth.com\\nLinkedin : https://fr.linkedin.com/in/sibylac\n",
      "{'form': 'Technicienne', 'label': 'Function', 'begin': 19, 'end': 31}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 27, 'end': 29}\n",
      "===============\n",
      "TEXT: Dorothée Charrette\\n94 rue du Faubourg National 94320 Thiais\\nCedex 02\\n+33 01 70 92 06 69 - dorotheecharette@outlook.com\\nEscrow Papers - escrowpapers.fr\\nPoste : Technicienne en téléphonie mobile\n",
      "{'form': 'Technicienne', 'label': 'Function', 'begin': 164, 'end': 176}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 172, 'end': 174}\n",
      "===============\n",
      "TEXT: Coralie Sanschagrin\\nVendeuse en magasin - Supermarket Lists\\nsupermarketlists.fr\\nAdresse : 12 place Maurice-Charretier 94220 Charenton-sur-le-Pont\\nTél : 0171220748\\nEmail : coraliesanschagrin@gmail.com\n",
      "{'form': 'Vendeuse', 'label': 'Function', 'begin': 21, 'end': 29}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 22, 'end': 24}\n",
      "===============\n",
      "TEXT: Octave Bernier\\nVendeur en boutique - Perles et Strass\\nperlesstrass.fr\\n01.07.16.35.66\\noctavebernier@outlook.com\\nAdresse : 75019 Paris France - 2 Faubourg Saint Honoré\n",
      "{'form': 'Vendeur', 'label': 'Function', 'begin': 16, 'end': 23}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 17, 'end': 19}\n",
      "===============\n",
      "TEXT: Landry Monrency\\n79 rue Grande Fusterie 91800 Brunot\\nTél : 01.18.60.73.47\\nEmail : landrymonrency@yahoo.fr\\nVendeur en Pharmacie - Pharmaplis\\npharmaplis.com\n",
      "{'form': 'Vendeur', 'label': 'Function', 'begin': 109, 'end': 116}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 110, 'end': 112}\n",
      "===============\n",
      "TEXT: Charles Voisine\\nTél : 04.28.88.77.19\\nEmail : charlesvoisine@gmail.com\\nAdresse : 51 boulevard d'Alsace 69120 Vaulx-en-Velin\\nSerrurier - Witmark\\nwitmark.com\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 86, 'end': 95}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 94, 'end': 95}\n",
      "===============\n",
      "TEXT: Monique Saucier\\nFrance, 92170 Vanves, 16 boulevard d'Alsace\\n01 35 90 80 12\\nmoniquesaucier@gmail.com\\nAfrican CDs\\nPoste : Vendeuse en boutique\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 42, 'end': 51}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 50, 'end': 51}\n",
      "===============\n",
      "TEXT: Monique Saucier\\nFrance, 92170 Vanves, 16 boulevard d'Alsace\\n01 35 90 80 12\\nmoniquesaucier@gmail.com\\nAfrican CDs\\nPoste : Vendeuse en boutique\n",
      "{'form': 'Vendeuse', 'label': 'Function', 'begin': 125, 'end': 133}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 126, 'end': 128}\n",
      "===============\n",
      "TEXT: Patrick Richard\\nGestionnaire de stocks / Laneco\\nTél : 01 09 18 31 44\\nAdresse : 78140 Vélizy-Villacoublay, France - 55 boulevard d'Alsace\\npatrichrichard@yahoo.fr\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 121, 'end': 130}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 129, 'end': 130}\n",
      "===============\n",
      "TEXT: Georges Lafontaine\\nCuisinier - Food Fair\\nAdresse : 70 boulevard d'Alsace 69200 Vénissieux\\nTél : 0419569361\\nEmail : georgeslafontaine@gmail.com\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 56, 'end': 65}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 64, 'end': 65}\n",
      "===============\n",
      "TEXT: Fabien Lejeune\\nTél : 03.82.43.44.54\\n34, boulevard d'Alsace, Verdun\\nPakla Hotel\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 42, 'end': 51}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 50, 'end': 51}\n",
      "===============\n",
      "TEXT: France Sicard\\nSicard Services\\n76 boulevard d'Alsace, Vanves\\nTél : 04 25 45 78 96\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 35, 'end': 44}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 43, 'end': 44}\n",
      "===============\n",
      "TEXT: Gaetan Desforges\\nArk Design\\n06.32.15.42.65\\nAdresse : 79 boulevard d'Alsace, Verdun\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 59, 'end': 68}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 67, 'end': 68}\n",
      "===============\n",
      "TEXT: Felipe Ramos\\nAdresse : Avinguda Soler 3-42\\nEl Juan - Colombie\\nTél : 995610494\\nGestora Rendón e Hijo\n",
      "{'form': 'Rendón', 'label': 'Organization', 'begin': 90, 'end': 96}\n",
      "{'form': 'e', 'label': 'Organization', 'begin': 91, 'end': 92}\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>jdf</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check_overlapping(data):\n",
    "    for data_row in data:\n",
    "        annotations = data_row['annotations']\n",
    "        for pre, post in zip(annotations[:-1], annotations[1:]):\n",
    "            if pre['end'] > post['begin']:\n",
    "                print('TEXT:', data_row['text'])\n",
    "                print(pre)\n",
    "                print(post)\n",
    "                print('='*15)\n",
    "\n",
    "def remove_overlapping(data):\n",
    "    for data_row in data:\n",
    "        annotations = data_row['annotations']\n",
    "        for pre, post in zip(annotations[:-1], annotations[1:]):\n",
    "            if pre['end'] >= post['end']:\n",
    "                annotations.remove(post)\n",
    "\n",
    "for split in data:\n",
    "    display(HTML('<h3>'+split+'</h3>'))\n",
    "    check_overlapping(data[split])\n",
    "\n",
    "for split in data:\n",
    "    remove_overlapping(data[split])\n",
    "    check_overlapping(data[split])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bef222-846e-453f-9cb0-6ca3733dbb71",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Est-ce qu'il existe un exemple qui manque d'annotation? Combien? Lesquels?</div>\n",
    "\n",
    "> Non, apparemment très cohérent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66c69c54-4a27-402e-b6e3-bd3d69eea537",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in data:\n",
    "    for data_row in data[split]:\n",
    "        annotations = data_row['annotations']\n",
    "        if len(annotations) == 0:\n",
    "            print(data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0aef61-f5a4-4b5b-9d3b-d6429f1203c4",
   "metadata": {},
   "source": [
    "## Tokenization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "31ea5d27-bc78-4087-ae91-d430b144c0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Faustin Chabot\\\\r\\\\nAdresse : 19 rue Descartes 94370 Sucy-en-Brie (France)\\\\r\\\\nCedex 9 CS 12468\\\\r\\\\nData Engineer / Algorithm XZ Project\\\\r\\\\nfaustinchabot@teleworm.com / Tel : +33 0134354919\\\\r\\\\nLinkedin : https://fr.linkedin.com/in/fauchab\\\\r\\\\nTeleworm France\\\\r\\\\nteleworm.france.com',\n",
       " 'Vallis Lachance\\\\r\\\\nConcepteur de publications web - Un Site, une BD\\\\r\\\\n14 rue Victor Hugo 60200 Compiègne\\\\r\\\\nCedex 12 CS 10202\\\\r\\\\nTel : 03.18.38.37.37\\\\r\\\\nEmail : vallislachance@monwax.com\\\\r\\\\nMonwax \\\\r\\\\nmonwax.com\\\\r\\\\nFacebook : https://www.facebook.com/vallislachance']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file sentencepiece.bpe.model from cache at .cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at .cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at .cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"camembert-base\",\n",
      "  \"architectures\": [\n",
      "    \"CamembertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁Faust', 'in', '▁Cha', 'bot', '\\\\', 'r', '\\\\', 'n', 'Ad', 'resse', '▁:', '▁19', '▁rue', '▁Descartes', '▁94', '370', '▁Suc', 'y', '-', 'en', '-', 'B', 'rie', '▁(', 'France', ')', '\\\\', 'r', '\\\\', 'n', 'Ce', 'dex', '▁9', '▁CS', '▁124', '68', '\\\\', 'r', '\\\\', 'n', 'D', 'ata', '▁Engine', 'er', '▁/', '▁Al', 'gor', 'ith', 'm', '▁X', 'Z', '▁Project', '\\\\', 'r', '\\\\', 'n', 'fa', 'ustin', 'cha', 'bot', '@', 'tel', 'e', 'w', 'orm', '.', 'com', '▁/', '▁Tel', '▁:', '▁+', '33', '▁01', '34', '35', '49', '19', '\\\\', 'r', '\\\\', 'n', 'L', 'ink', 'e', 'din', '▁:', '▁https', '://', 'fr', '.', 'link', 'e', 'din', '.', 'com', '/', 'in', '/', 'fa', 'uch', 'ab', '\\\\', 'r', '\\\\', 'n', 'T', 'ele', 'w', 'orm', '▁France', '\\\\', 'r', '\\\\', 'nt', 'ele', 'w', 'orm', '.', 'france', '.', 'com', '</s>']\n",
      "['<s>', '▁Val', 'lis', '▁La', 'ch', 'ance', '\\\\', 'r', '\\\\', 'n', 'Con', 'cept', 'eur', '▁de', '▁publications', '▁web', '▁-', '▁Un', '▁Site', ',', '▁une', '▁BD', '\\\\', 'r', '\\\\', 'n', '14', '▁rue', '▁Victor', '▁Hugo', '▁60', '200', '▁Compiègne', '\\\\', 'r', '\\\\', 'n', 'Ce', 'dex', '▁12', '▁CS', '▁102', '02', '\\\\', 'r', '\\\\', 'n', 'T', 'el', '▁:', '▁03', '.', '18', '.', '38', '.', '37', '.', '37', '\\\\', 'r', '\\\\', 'n', 'E', 'mail', '▁:', '▁val', 'lis', 'la', 'ch', 'ance', '@', 'mon', 'wa', 'x', '.', 'com', '\\\\', 'r', '\\\\', 'n', 'Mon', 'wa', 'x', '▁\\\\', 'r', '\\\\', 'n', 'mon', 'wa', 'x', '.', 'com', '\\\\', 'r', '\\\\', 'n', 'F', 'ace', 'book', '▁:', '▁https', '://', 'www', '.', 'facebook', '.', 'com', '/', 'val', 'lis', 'la', 'ch', 'ance', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizerFast\n",
    "\n",
    "MAX_LINE = 1000000\n",
    "CACHE_DIR = path.join('.cache', 'transformers')\n",
    "\n",
    "\n",
    "texts = [d['text'] for d in data['jdr'][:MAX_LINE]]\n",
    "print('Testing text:')\n",
    "display(texts[:2])\n",
    "\n",
    "#fast_tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\", cache_dir=CACHE_DIR, additional_special_tokens=['\\\\n', '\\\\r', 'https://'])\n",
    "fast_tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\", cache_dir=CACHE_DIR)\n",
    "\n",
    "token_encodings = fast_tokenizer(texts, return_offsets_mapping=True)\n",
    "\n",
    "token_strings = [fast_tokenizer.convert_ids_to_tokens(input_ids) for input_ids in token_encodings.input_ids]\n",
    "\n",
    "for tkstr in token_strings[:2]:\n",
    "    print(tkstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "66954b44-43bb-4a1c-844d-6742f3214015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2label = ['I-Reference_CS', 'I-Reference_User', 'B-Reference_User', 'B-Organization', 'B-Reference_CS', 'I-Human', 'I-Email', 'O', 'B-Human', 'B-Project', 'I-Social_Network', 'I-Reference_CEDEX', 'I-Url', 'B-Url', 'I-Function', 'I-Location', 'B-Email', 'B-Social_Network', 'B-Reference_CEDEX', 'B-Phone_Number', 'B-Location', 'B-Function', 'I-Phone_Number', 'I-Organization', 'I-Reference_Code_Postal', 'I-Project', 'B-Reference_Code_Postal']\n",
      "label2id = {'I-Reference_CS': 0, 'I-Reference_User': 1, 'B-Reference_User': 2, 'B-Organization': 3, 'B-Reference_CS': 4, 'I-Human': 5, 'I-Email': 6, 'O': 7, 'B-Human': 8, 'B-Project': 9, 'I-Social_Network': 10, 'I-Reference_CEDEX': 11, 'I-Url': 12, 'B-Url': 13, 'I-Function': 14, 'I-Location': 15, 'B-Email': 16, 'B-Social_Network': 17, 'B-Reference_CEDEX': 18, 'B-Phone_Number': 19, 'B-Location': 20, 'B-Function': 21, 'I-Phone_Number': 22, 'I-Organization': 23, 'I-Reference_Code_Postal': 24, 'I-Project': 25, 'B-Reference_Code_Postal': 26}\n"
     ]
    }
   ],
   "source": [
    "def mapping_label_token(token_span_batch, annotations_batch):\n",
    "    \"\"\"\n",
    "    Remap IOB tag to each token generated by tokenizer. Should provide the span (begin/end)\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = list()\n",
    "    \n",
    "    for token_span_sent, annotations in zip(token_span_batch, annotations_batch):\n",
    "        \n",
    "        annotations = annotations.copy()\n",
    "        entity = annotations.pop(0)\n",
    "        \n",
    "        last_label = 'O'\n",
    "        token_label = list()\n",
    "        \n",
    "        for token in token_span_sent:\n",
    "            \n",
    "            while entity['end'] < token['begin']: entity = annotations.pop(0)\n",
    "\n",
    "            if token['begin'] == token['end']:\n",
    "                label = 'O'    \n",
    "            elif entity['begin'] <= token['begin'] and token['end'] <= entity['end']:\n",
    "                prefix = 'B-' if last_label == 'O' or last_label[2:] != entity['label'] else 'I-'\n",
    "                label = prefix + entity['label']\n",
    "            else:\n",
    "                label = 'O'\n",
    "                \n",
    "            token_label.append(label)\n",
    "            last_label = label\n",
    "                \n",
    "        labels.append(token_label)\n",
    "        \n",
    "    return labels\n",
    "\n",
    "def tokenize_text(texts, annotations, tokenizer):\n",
    "    \n",
    "    # Tokenize text\n",
    "    token_encodings = tokenizer(texts, return_offsets_mapping=True)\n",
    "    token_encodings['tokens'] = [fast_tokenizer.convert_ids_to_tokens(input_ids) for input_ids in token_encodings.input_ids]\n",
    "    \n",
    "    # Mapping labels\n",
    "    token_span = token_encodings.offset_mapping\n",
    "    token_span_dict = [[{'begin': span[0], 'end': span[1]} for span in token_sent ] for token_sent in token_span]\n",
    "    token_encodings['ner_tags'] = mapping_label_token(token_span_dict, annotations)\n",
    "    \n",
    "    return token_encodings\n",
    "\n",
    "annotations = [d['annotations'] for d in data['jdr'][:MAX_LINE]]\n",
    "texts = [d['text'] for d in data['jdr'][:MAX_LINE]]\n",
    "tokenized = tokenize_text(texts, annotations, fast_tokenizer)\n",
    "\n",
    "all_labels = [i for l in tokenized['ner_tags'] for i in l ]\n",
    "unique_label = set(all_labels)\n",
    "id2label = list(set(all_labels))\n",
    "print('id2label =',id2label)\n",
    "label2id = {label: idx for idx, label in enumerate(id2label)}\n",
    "print('label2id =',label2id)\n",
    "\n",
    "tokenized['labels'] = [[label2id[label] for label in label_sentence] for label_sentence in tokenized['ner_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a92e95c1-8b9a-464c-9781-e36ba26944b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(tokenized))\n",
    "os.makedirs(path.join('..', '.cache'), exist_ok=True)\n",
    "df.to_csv(path.join('..','.cache', 'jdr.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d18a9ff0-3b91-47fc-a341-44dd5e3483b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>offset_mapping</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 28119, 236, 2614, 8674, 3155, 81, 3155, 25...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 5), (5, 7), (8, 11), (11, 14), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Faust, in, ▁Cha, bot, \\, r, \\, n, Ad, r...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 20, 15, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5, 1598, 4026, 61, 751, 1269, 3155, 81, 3155,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 3), (3, 6), (7, 9), (9, 11), (11,...</td>\n",
       "      <td>[&lt;s&gt;, ▁Val, lis, ▁La, ch, ance, \\, r, \\, n, Co...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, I-Huma...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 5, 7, 7, 7, 7, 21, 14, 14, 14,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5, 11904, 73, 6445, 276, 8348, 88, 3155, 81, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 4), (4, 6), (6, 8), (8, 10), (11,...</td>\n",
       "      <td>[&lt;s&gt;, ▁Arch, ai, mb, au, ▁Mass, on, \\, r, \\, n...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, I-Huma...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 5, 5, 7, 7, 7, 7, 21, 14, 14, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5, 470, 1606, 9313, 2265, 3155, 81, 3155, 255...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 4), (4, 8), (9, 12), (12, 16), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Jean, ette, ▁Fre, mont, \\, r, \\, n, 8, ...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 7, 7, 7, 7, 20, 15, 15, 15, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5, 3696, 19483, 236, 3155, 81, 3155, 255, 137...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 4), (5, 8), (8, 10), (10, 11), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Cher, ▁Baz, in, \\, r, \\, n, Mé, can, ic...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, O, O, O, O, B-F...</td>\n",
       "      <td>[7, 8, 5, 5, 7, 7, 7, 7, 21, 14, 14, 14, 14, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>[5, 4114, 61, 29807, 3155, 255, 6179, 7148, 43...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 6), (7, 9), (9, 13), (13, 14), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Claude, ▁La, ndry, \\, n, Ad, resse, ▁:,...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, O, O, O, O, O, ...</td>\n",
       "      <td>[7, 8, 5, 5, 7, 7, 7, 7, 7, 20, 15, 15, 15, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>[5, 11853, 9625, 10, 1981, 3155, 255, 3853, 30...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 9), (10, 13), (13, 14), (14, 18),...</td>\n",
       "      <td>[&lt;s&gt;, ▁Charlotte, ▁Bus, s, ière, \\, n, 59, ▁co...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 7, 7, 20, 15, 15, 15, 15, 26, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>[5, 18467, 24817, 3155, 255, 3225, 9, 3220, 9,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 6), (7, 14), (14, 15), (15, 16), ...</td>\n",
       "      <td>[&lt;s&gt;, ▁Cédric, ▁Garnier, \\, n, 04, ., 27, ., 1...</td>\n",
       "      <td>[O, B-Human, I-Human, O, O, B-Phone_Number, I-...</td>\n",
       "      <td>[7, 8, 5, 7, 7, 19, 22, 22, 22, 22, 22, 22, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>[5, 14147, 10223, 11734, 4461, 3155, 255, 3395...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 3), (3, 9), (10, 13), (13, 16), (...</td>\n",
       "      <td>[&lt;s&gt;, ▁Fla, vienne, ▁Dev, ost, \\, n, 02, ., 56...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 7, 7, 19, 22, 22, 22, 22, 22, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>[5, 19937, 487, 10340, 2872, 3155, 255, 585, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 6), (6, 8), (9, 12), (12, 16), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Violet, te, ▁Fau, bert, \\, n, O, m, ni,...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 7, 7, 3, 23, 23, 23, 23, 7, 7,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>473 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_ids  \\\n",
       "0    [5, 28119, 236, 2614, 8674, 3155, 81, 3155, 25...   \n",
       "1    [5, 1598, 4026, 61, 751, 1269, 3155, 81, 3155,...   \n",
       "2    [5, 11904, 73, 6445, 276, 8348, 88, 3155, 81, ...   \n",
       "3    [5, 470, 1606, 9313, 2265, 3155, 81, 3155, 255...   \n",
       "4    [5, 3696, 19483, 236, 3155, 81, 3155, 255, 137...   \n",
       "..                                                 ...   \n",
       "468  [5, 4114, 61, 29807, 3155, 255, 6179, 7148, 43...   \n",
       "469  [5, 11853, 9625, 10, 1981, 3155, 255, 3853, 30...   \n",
       "470  [5, 18467, 24817, 3155, 255, 3225, 9, 3220, 9,...   \n",
       "471  [5, 14147, 10223, 11734, 4461, 3155, 255, 3395...   \n",
       "472  [5, 19937, 487, 10340, 2872, 3155, 255, 585, 2...   \n",
       "\n",
       "                                        attention_mask  \\\n",
       "0    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "..                                                 ...   \n",
       "468  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "469  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "470  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "471  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "472  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                        offset_mapping  \\\n",
       "0    [(0, 0), (0, 5), (5, 7), (8, 11), (11, 14), (1...   \n",
       "1    [(0, 0), (0, 3), (3, 6), (7, 9), (9, 11), (11,...   \n",
       "2    [(0, 0), (0, 4), (4, 6), (6, 8), (8, 10), (11,...   \n",
       "3    [(0, 0), (0, 4), (4, 8), (9, 12), (12, 16), (1...   \n",
       "4    [(0, 0), (0, 4), (5, 8), (8, 10), (10, 11), (1...   \n",
       "..                                                 ...   \n",
       "468  [(0, 0), (0, 6), (7, 9), (9, 13), (13, 14), (1...   \n",
       "469  [(0, 0), (0, 9), (10, 13), (13, 14), (14, 18),...   \n",
       "470  [(0, 0), (0, 6), (7, 14), (14, 15), (15, 16), ...   \n",
       "471  [(0, 0), (0, 3), (3, 9), (10, 13), (13, 16), (...   \n",
       "472  [(0, 0), (0, 6), (6, 8), (9, 12), (12, 16), (1...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [<s>, ▁Faust, in, ▁Cha, bot, \\, r, \\, n, Ad, r...   \n",
       "1    [<s>, ▁Val, lis, ▁La, ch, ance, \\, r, \\, n, Co...   \n",
       "2    [<s>, ▁Arch, ai, mb, au, ▁Mass, on, \\, r, \\, n...   \n",
       "3    [<s>, ▁Jean, ette, ▁Fre, mont, \\, r, \\, n, 8, ...   \n",
       "4    [<s>, ▁Cher, ▁Baz, in, \\, r, \\, n, Mé, can, ic...   \n",
       "..                                                 ...   \n",
       "468  [<s>, ▁Claude, ▁La, ndry, \\, n, Ad, resse, ▁:,...   \n",
       "469  [<s>, ▁Charlotte, ▁Bus, s, ière, \\, n, 59, ▁co...   \n",
       "470  [<s>, ▁Cédric, ▁Garnier, \\, n, 04, ., 27, ., 1...   \n",
       "471  [<s>, ▁Fla, vienne, ▁Dev, ost, \\, n, 02, ., 56...   \n",
       "472  [<s>, ▁Violet, te, ▁Fau, bert, \\, n, O, m, ni,...   \n",
       "\n",
       "                                              ner_tags  \\\n",
       "0    [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "1    [O, B-Human, I-Human, I-Human, I-Human, I-Huma...   \n",
       "2    [O, B-Human, I-Human, I-Human, I-Human, I-Huma...   \n",
       "3    [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "4    [O, B-Human, I-Human, I-Human, O, O, O, O, B-F...   \n",
       "..                                                 ...   \n",
       "468  [O, B-Human, I-Human, I-Human, O, O, O, O, O, ...   \n",
       "469  [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "470  [O, B-Human, I-Human, O, O, B-Phone_Number, I-...   \n",
       "471  [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "472  [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "\n",
       "                                                labels  \n",
       "0    [7, 8, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 20, 15, 1...  \n",
       "1    [7, 8, 5, 5, 5, 5, 7, 7, 7, 7, 21, 14, 14, 14,...  \n",
       "2    [7, 8, 5, 5, 5, 5, 5, 7, 7, 7, 7, 21, 14, 14, ...  \n",
       "3    [7, 8, 5, 5, 5, 7, 7, 7, 7, 20, 15, 15, 15, 15...  \n",
       "4    [7, 8, 5, 5, 7, 7, 7, 7, 21, 14, 14, 14, 14, 7...  \n",
       "..                                                 ...  \n",
       "468  [7, 8, 5, 5, 7, 7, 7, 7, 7, 20, 15, 15, 15, 15...  \n",
       "469  [7, 8, 5, 5, 5, 7, 7, 20, 15, 15, 15, 15, 26, ...  \n",
       "470  [7, 8, 5, 7, 7, 19, 22, 22, 22, 22, 22, 22, 22...  \n",
       "471  [7, 8, 5, 5, 5, 7, 7, 19, 22, 22, 22, 22, 22, ...  \n",
       "472  [7, 8, 5, 5, 5, 7, 7, 3, 23, 23, 23, 23, 7, 7,...  \n",
       "\n",
       "[473 rows x 6 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec73e8b0-713e-4aa6-ae1b-75a0895524b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextMineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, split):\n",
    "        self.split = split\n",
    "        \n",
    "        self.data = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self): raise IndexError  # meet the end of dataset\n",
    "        sample = self.data.loc[idx].to_dict()\n",
    "        #for k, v in sample.items():\n",
    "        #   print(k, '=', len(v))\n",
    "        return {'input_ids': sample['input_ids'], 'attention_mask': sample['attention_mask'],'labels': sample['labels']}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "81fafc63-f55f-4927-9a4e-f68dc4e990c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at .cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"camembert-base\",\n",
      "  \"architectures\": [\n",
      "    \"CamembertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": [\n",
      "    \"I-Reference_CS\",\n",
      "    \"I-Reference_User\",\n",
      "    \"B-Reference_User\",\n",
      "    \"B-Organization\",\n",
      "    \"B-Reference_CS\",\n",
      "    \"I-Human\",\n",
      "    \"I-Email\",\n",
      "    \"O\",\n",
      "    \"B-Human\",\n",
      "    \"B-Project\",\n",
      "    \"I-Social_Network\",\n",
      "    \"I-Reference_CEDEX\",\n",
      "    \"I-Url\",\n",
      "    \"B-Url\",\n",
      "    \"I-Function\",\n",
      "    \"I-Location\",\n",
      "    \"B-Email\",\n",
      "    \"B-Social_Network\",\n",
      "    \"B-Reference_CEDEX\",\n",
      "    \"B-Phone_Number\",\n",
      "    \"B-Location\",\n",
      "    \"B-Function\",\n",
      "    \"I-Phone_Number\",\n",
      "    \"I-Organization\",\n",
      "    \"I-Reference_Code_Postal\",\n",
      "    \"I-Project\",\n",
      "    \"B-Reference_Code_Postal\"\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-Email\": 16,\n",
      "    \"B-Function\": 21,\n",
      "    \"B-Human\": 8,\n",
      "    \"B-Location\": 20,\n",
      "    \"B-Organization\": 3,\n",
      "    \"B-Phone_Number\": 19,\n",
      "    \"B-Project\": 9,\n",
      "    \"B-Reference_CEDEX\": 18,\n",
      "    \"B-Reference_CS\": 4,\n",
      "    \"B-Reference_Code_Postal\": 26,\n",
      "    \"B-Reference_User\": 2,\n",
      "    \"B-Social_Network\": 17,\n",
      "    \"B-Url\": 13,\n",
      "    \"I-Email\": 6,\n",
      "    \"I-Function\": 14,\n",
      "    \"I-Human\": 5,\n",
      "    \"I-Location\": 15,\n",
      "    \"I-Organization\": 23,\n",
      "    \"I-Phone_Number\": 22,\n",
      "    \"I-Project\": 25,\n",
      "    \"I-Reference_CEDEX\": 11,\n",
      "    \"I-Reference_CS\": 0,\n",
      "    \"I-Reference_Code_Postal\": 24,\n",
      "    \"I-Reference_User\": 1,\n",
      "    \"I-Social_Network\": 10,\n",
      "    \"I-Url\": 12,\n",
      "    \"O\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at .cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/pytorch_model.bin\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [87], line 72\u001b[0m\n\u001b[1;32m     57\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamembert-base\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(id2label), id2label\u001b[38;5;241m=\u001b[39mid2label, label2id\u001b[38;5;241m=\u001b[39mlabel2id, cache_dir\u001b[38;5;241m=\u001b[39mCACHE_DIR)\n\u001b[1;32m     58\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForTokenClassification(fast_tokenizer)\n\u001b[1;32m     60\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     61\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,  \u001b[38;5;66;03m# batch size per device during training\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,   \u001b[38;5;66;03m# batch size for evaluation\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     64\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.cache\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     65\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.cache\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     66\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     67\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     69\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     70\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     71\u001b[0m         EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m---> 72\u001b[0m         \u001b[43mTensorBoardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     73\u001b[0m     ]\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     75\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     76\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     77\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     85\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/transformers/integrations.py:562\u001b[0m, in \u001b[0;36mTensorBoardCallback.__init__\u001b[0;34m(self, tb_writer)\u001b[0m\n\u001b[1;32m    560\u001b[0m has_tensorboard \u001b[38;5;241m=\u001b[39m is_tensorboard_available()\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tensorboard:\n\u001b[0;32m--> 562\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m install tensorboardX.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_tensorboard:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForTokenClassification, EarlyStoppingCallback\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextMineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.data = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self): raise IndexError  # meet the end of dataset\n",
    "        sample = self.data.loc[idx].to_dict()\n",
    "        #for k, v in sample.items():\n",
    "        #   print(k, '=', len(v))\n",
    "        return {'input_ids': sample['input_ids'], 'attention_mask': sample['attention_mask'],'labels': sample['labels']}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "dataset = TextMineDataset(df)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "        if(k not in flattened_results.keys()):\n",
    "            flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"camembert-base\", num_labels=len(id2label), id2label=id2label, label2id=label2id, cache_dir=CACHE_DIR)\n",
    "data_collator = DataCollatorForTokenClassification(fast_tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    num_train_epochs=20,\n",
    "    output_dir=path.join('..', '.cache', 'results'),\n",
    "    logging_dir=path.join('..', '.cache', 'logs'),\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "        TensorBoardCallback(),\n",
    "    ]\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=fast_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e3c3e-9c17-4b40-9988-3b9f8057f5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
