{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991db548-8934-44fe-9608-3a581720de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "sys.path.append(\"./../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a78f401-60b1-40d0-b8a2-94b1da80845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JDR #examples : 473\n",
      "JDF #examples : 500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Les entités nommées</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>label</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Faustin</td>\n",
       "      <td>Human</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chabot</td>\n",
       "      <td>Human</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>Location</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rue</td>\n",
       "      <td>Location</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Descartes</td>\n",
       "      <td>Location</td>\n",
       "      <td>35</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        form     label  begin  end\n",
       "0    Faustin     Human      0    7\n",
       "1     Chabot     Human      8   14\n",
       "2         19  Location     28   30\n",
       "3        rue  Location     31   34\n",
       "4  Descartes  Location     35   44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Human', 'Location', 'Reference_Code_Postal', 'Reference_CEDEX', 'Reference_CS', ..., 'Phone_Number', 'Social_Network', 'Reference_User', 'Organization', 'Url']\n",
      "Length: 13\n",
      "Categories (13, object): ['Email', 'Function', 'Human', 'Location', ..., 'Reference_Code_Postal', 'Reference_User', 'Social_Network', 'Url']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "DATA_PATH = path.join('..', 'dataset')\n",
    "JDF_PATH = path.join(DATA_PATH, 'JDF.json')\n",
    "JDR_PATH = path.join(DATA_PATH, 'JDR.json')\n",
    "\n",
    "data = dict()\n",
    "\n",
    "with open(JDR_PATH, 'r') as f:\n",
    "    data['jdr'] = json.load(f)\n",
    "    \n",
    "with open(JDF_PATH, 'r') as f:\n",
    "    data['jdf'] = json.load(f)\n",
    "\n",
    "print('JDR #examples :',len(data['jdr']))\n",
    "print('JDF #examples :',len(data['jdf']))\n",
    "\n",
    "annotations = [a for d in data['jdr'] for a in d['annotations']]\n",
    "df_annotations = pd.DataFrame(annotations)\n",
    "df_annotations[\"label\"] = df_annotations[\"label\"].astype(\"category\")\n",
    "display(HTML('<h3>Les entités nommées</h3>'))\n",
    "display(df_annotations.head())\n",
    "\n",
    "print('Labels:', df_annotations['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab065a9-153a-41be-8c8b-8b1a68e4dff2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Format d'une pharse donnée</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a14ce8-46de-4929-97bc-94b7dcb2dfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'identifier': 500,\n",
       " 'text': 'Faustin Chabot\\\\r\\\\nAdresse : 19 rue Descartes 94370 Sucy-en-Brie (France)\\\\r\\\\nCedex 9 CS 12468\\\\r\\\\nData Engineer / Algorithm XZ Project\\\\r\\\\nfaustinchabot@teleworm.com / Tel : +33 0134354919\\\\r\\\\nLinkedin : https://fr.linkedin.com/in/fauchab\\\\r\\\\nTeleworm France\\\\r\\\\nteleworm.france.com',\n",
       " 'annotations': [{'form': 'Faustin', 'label': 'Human', 'begin': 0, 'end': 7},\n",
       "  {'form': 'Chabot', 'label': 'Human', 'begin': 8, 'end': 14},\n",
       "  {'form': '19', 'label': 'Location', 'begin': 28, 'end': 30},\n",
       "  {'form': 'rue', 'label': 'Location', 'begin': 31, 'end': 34},\n",
       "  {'form': 'Descartes', 'label': 'Location', 'begin': 35, 'end': 44},\n",
       "  {'form': '94370', 'label': 'Reference_Code_Postal', 'begin': 45, 'end': 50},\n",
       "  {'form': 'Sucy-en-Brie', 'label': 'Location', 'begin': 51, 'end': 63},\n",
       "  {'form': 'France', 'label': 'Location', 'begin': 65, 'end': 71},\n",
       "  {'form': 'Cedex', 'label': 'Reference_CEDEX', 'begin': 76, 'end': 81},\n",
       "  {'form': '9', 'label': 'Reference_CEDEX', 'begin': 82, 'end': 83},\n",
       "  {'form': 'CS', 'label': 'Reference_CS', 'begin': 84, 'end': 86},\n",
       "  {'form': '12468', 'label': 'Reference_CS', 'begin': 87, 'end': 92},\n",
       "  {'form': 'Data', 'label': 'Function', 'begin': 96, 'end': 100},\n",
       "  {'form': 'Engineer', 'label': 'Function', 'begin': 101, 'end': 109},\n",
       "  {'form': 'Algorithm', 'label': 'Project', 'begin': 112, 'end': 121},\n",
       "  {'form': 'XZ', 'label': 'Project', 'begin': 122, 'end': 124},\n",
       "  {'form': 'Project', 'label': 'Project', 'begin': 125, 'end': 132},\n",
       "  {'form': 'faustinchabot@teleworm.com',\n",
       "   'label': 'Email',\n",
       "   'begin': 136,\n",
       "   'end': 162},\n",
       "  {'form': '+33 0134354919',\n",
       "   'label': 'Phone_Number',\n",
       "   'begin': 171,\n",
       "   'end': 185},\n",
       "  {'form': 'Linkedin', 'label': 'Social_Network', 'begin': 189, 'end': 197},\n",
       "  {'form': 'https://fr.linkedin.com/in/fauchab',\n",
       "   'label': 'Reference_User',\n",
       "   'begin': 200,\n",
       "   'end': 234},\n",
       "  {'form': 'Teleworm', 'label': 'Organization', 'begin': 238, 'end': 246},\n",
       "  {'form': 'France', 'label': 'Organization', 'begin': 247, 'end': 253},\n",
       "  {'form': 'teleworm.france.com', 'label': 'Url', 'begin': 257, 'end': 276}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['jdr'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8240750-6ed2-4bf7-affa-d81db22eae02",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Est-ce qu'il y a de motif pour le numéro de téléphone?</div>\n",
    "\n",
    "> Apparemment non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538bdd22-5edd-488f-9e6d-57c720ddb879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>label</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>+33 0134354919</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>171</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>03.18.38.37.37</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>136</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>+ 03 81 20 48 27</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>152</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>01.75.88.25.30</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>77</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>+ 33 01 77 83 74 05</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>157</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>+33 0365962110</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>70</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>01.55.29.21.75</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>118</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>+ 33 01 79 28 30 87</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>75</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>33 0147908347</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>150</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>03.54.57.86.42</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>162</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>+ 33 0529308213</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>113</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>03 46 69 07 08</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>121</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>+ 33 0221446127</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>129</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>01.92.26.82.75</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>124</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>01.17.82.71.60</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>151</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>+33 01.26.31.62.25</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>86</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>02 66 69 43 14</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>143</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>02.96.95.55.61</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>146</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>+ 33 01.47.04.39.40</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>134</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>+33 04.12.26.56.34</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>90</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    form         label  begin  end\n",
       "18        +33 0134354919  Phone_Number    171  185\n",
       "45        03.18.38.37.37  Phone_Number    136  150\n",
       "72      + 03 81 20 48 27  Phone_Number    152  168\n",
       "91        01.75.88.25.30  Phone_Number     77   91\n",
       "122  + 33 01 77 83 74 05  Phone_Number    157  176\n",
       "134       +33 0365962110  Phone_Number     70   84\n",
       "170       01.55.29.21.75  Phone_Number    118  132\n",
       "191  + 33 01 79 28 30 87  Phone_Number     75   94\n",
       "225        33 0147908347  Phone_Number    150  163\n",
       "250       03.54.57.86.42  Phone_Number    162  176\n",
       "269      + 33 0529308213  Phone_Number    113  128\n",
       "291       03 46 69 07 08  Phone_Number    121  135\n",
       "315      + 33 0221446127  Phone_Number    129  144\n",
       "333       01.92.26.82.75  Phone_Number    124  138\n",
       "364       01.17.82.71.60  Phone_Number    151  165\n",
       "378   +33 01.26.31.62.25  Phone_Number     86  104\n",
       "418       02 66 69 43 14  Phone_Number    143  157\n",
       "443       02.96.95.55.61  Phone_Number    146  160\n",
       "465  + 33 01.47.04.39.40  Phone_Number    134  153\n",
       "482   +33 04.12.26.56.34  Phone_Number     90  108"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations[df_annotations['label']=='Phone_Number'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae549953-f48a-4da9-9b86-9c14f7c16f64",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Liste des étiquettes à prédire</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b3bec3-c292-4e58-a83e-63a58ab880c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human',\n",
       " 'Location',\n",
       " 'Reference_Code_Postal',\n",
       " 'Reference_CEDEX',\n",
       " 'Reference_CS',\n",
       " 'Function',\n",
       " 'Project',\n",
       " 'Email',\n",
       " 'Phone_Number',\n",
       " 'Social_Network',\n",
       " 'Reference_User',\n",
       " 'Organization',\n",
       " 'Url']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_annotations['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95b0a9-07e5-47b0-b7aa-2c92694c09ab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Les étiquettes sont-elles chevauchées?</div>\n",
    "\n",
    "> Oui, il semble un bruit dans l'outil d'annotation. Il suffit de surrprimer celui contenu dans la vraie étiquette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7e5f47-c02e-494a-8949-515b7f5c9e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>jdr</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: Sibyla Chandonnet\\nTechnicienne en radiologie / Health For All\\nTel : +33 0365962110 \\nsibylachandonnet@yahoo.fr\\n89 rue du Général Ailleret 62300 Lens\\nCedex 08 CS 40362\\nThe Happy Bear / happybearhealth.com\\nLinkedin : https://fr.linkedin.com/in/sibylac\n",
      "{'form': 'Technicienne', 'label': 'Function', 'begin': 19, 'end': 31}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 27, 'end': 29}\n",
      "===============\n",
      "TEXT: Dorothée Charrette\\n94 rue du Faubourg National 94320 Thiais\\nCedex 02\\n+33 01 70 92 06 69 - dorotheecharette@outlook.com\\nEscrow Papers - escrowpapers.fr\\nPoste : Technicienne en téléphonie mobile\n",
      "{'form': 'Technicienne', 'label': 'Function', 'begin': 164, 'end': 176}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 172, 'end': 174}\n",
      "===============\n",
      "TEXT: Coralie Sanschagrin\\nVendeuse en magasin - Supermarket Lists\\nsupermarketlists.fr\\nAdresse : 12 place Maurice-Charretier 94220 Charenton-sur-le-Pont\\nTél : 0171220748\\nEmail : coraliesanschagrin@gmail.com\n",
      "{'form': 'Vendeuse', 'label': 'Function', 'begin': 21, 'end': 29}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 22, 'end': 24}\n",
      "===============\n",
      "TEXT: Octave Bernier\\nVendeur en boutique - Perles et Strass\\nperlesstrass.fr\\n01.07.16.35.66\\noctavebernier@outlook.com\\nAdresse : 75019 Paris France - 2 Faubourg Saint Honoré\n",
      "{'form': 'Vendeur', 'label': 'Function', 'begin': 16, 'end': 23}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 17, 'end': 19}\n",
      "===============\n",
      "TEXT: Landry Monrency\\n79 rue Grande Fusterie 91800 Brunot\\nTél : 01.18.60.73.47\\nEmail : landrymonrency@yahoo.fr\\nVendeur en Pharmacie - Pharmaplis\\npharmaplis.com\n",
      "{'form': 'Vendeur', 'label': 'Function', 'begin': 109, 'end': 116}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 110, 'end': 112}\n",
      "===============\n",
      "TEXT: Charles Voisine\\nTél : 04.28.88.77.19\\nEmail : charlesvoisine@gmail.com\\nAdresse : 51 boulevard d'Alsace 69120 Vaulx-en-Velin\\nSerrurier - Witmark\\nwitmark.com\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 86, 'end': 95}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 94, 'end': 95}\n",
      "===============\n",
      "TEXT: Monique Saucier\\nFrance, 92170 Vanves, 16 boulevard d'Alsace\\n01 35 90 80 12\\nmoniquesaucier@gmail.com\\nAfrican CDs\\nPoste : Vendeuse en boutique\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 42, 'end': 51}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 50, 'end': 51}\n",
      "===============\n",
      "TEXT: Monique Saucier\\nFrance, 92170 Vanves, 16 boulevard d'Alsace\\n01 35 90 80 12\\nmoniquesaucier@gmail.com\\nAfrican CDs\\nPoste : Vendeuse en boutique\n",
      "{'form': 'Vendeuse', 'label': 'Function', 'begin': 125, 'end': 133}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 126, 'end': 128}\n",
      "===============\n",
      "TEXT: Patrick Richard\\nGestionnaire de stocks / Laneco\\nTél : 01 09 18 31 44\\nAdresse : 78140 Vélizy-Villacoublay, France - 55 boulevard d'Alsace\\npatrichrichard@yahoo.fr\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 121, 'end': 130}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 129, 'end': 130}\n",
      "===============\n",
      "TEXT: Georges Lafontaine\\nCuisinier - Food Fair\\nAdresse : 70 boulevard d'Alsace 69200 Vénissieux\\nTél : 0419569361\\nEmail : georgeslafontaine@gmail.com\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 56, 'end': 65}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 64, 'end': 65}\n",
      "===============\n",
      "TEXT: Fabien Lejeune\\nTél : 03.82.43.44.54\\n34, boulevard d'Alsace, Verdun\\nPakla Hotel\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 42, 'end': 51}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 50, 'end': 51}\n",
      "===============\n",
      "TEXT: France Sicard\\nSicard Services\\n76 boulevard d'Alsace, Vanves\\nTél : 04 25 45 78 96\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 35, 'end': 44}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 43, 'end': 44}\n",
      "===============\n",
      "TEXT: Gaetan Desforges\\nArk Design\\n06.32.15.42.65\\nAdresse : 79 boulevard d'Alsace, Verdun\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 59, 'end': 68}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 67, 'end': 68}\n",
      "===============\n",
      "TEXT: Felipe Ramos\\nAdresse : Avinguda Soler 3-42\\nEl Juan - Colombie\\nTél : 995610494\\nGestora Rendón e Hijo\n",
      "{'form': 'Rendón', 'label': 'Organization', 'begin': 90, 'end': 96}\n",
      "{'form': 'e', 'label': 'Organization', 'begin': 91, 'end': 92}\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>jdf</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check_overlapping(data):\n",
    "    for data_row in data:\n",
    "        annotations = data_row['annotations']\n",
    "        for pre, post in zip(annotations[:-1], annotations[1:]):\n",
    "            if pre['end'] > post['begin']:\n",
    "                print('TEXT:', data_row['text'])\n",
    "                print(pre)\n",
    "                print(post)\n",
    "                print('='*15)\n",
    "\n",
    "def remove_overlapping(data):\n",
    "    for data_row in data:\n",
    "        annotations = data_row['annotations']\n",
    "        for pre, post in zip(annotations[:-1], annotations[1:]):\n",
    "            if pre['end'] >= post['end']:\n",
    "                annotations.remove(post)\n",
    "\n",
    "for split in data:\n",
    "    display(HTML('<h3>'+split+'</h3>'))\n",
    "    check_overlapping(data[split])\n",
    "\n",
    "for split in data:\n",
    "    remove_overlapping(data[split])\n",
    "    check_overlapping(data[split])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bef222-846e-453f-9cb0-6ca3733dbb71",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Est-ce qu'il existe un exemple qui manque d'annotation? Combien? Lesquels?</div>\n",
    "\n",
    "> Non, apparemment très cohérent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c69c54-4a27-402e-b6e3-bd3d69eea537",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in data:\n",
    "    for data_row in data[split]:\n",
    "        annotations = data_row['annotations']\n",
    "        if len(annotations) == 0:\n",
    "            print(data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0aef61-f5a4-4b5b-9d3b-d6429f1203c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tokenization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31ea5d27-bc78-4087-ae91-d430b144c0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Faustin Chabot\\\\r\\\\nAdresse : 19 rue Descartes 94370 Sucy-en-Brie (France)\\\\r\\\\nCedex 9 CS 12468\\\\r\\\\nData Engineer / Algorithm XZ Project\\\\r\\\\nfaustinchabot@teleworm.com / Tel : +33 0134354919\\\\r\\\\nLinkedin : https://fr.linkedin.com/in/fauchab\\\\r\\\\nTeleworm France\\\\r\\\\nteleworm.france.com',\n",
       " 'Vallis Lachance\\\\r\\\\nConcepteur de publications web - Un Site, une BD\\\\r\\\\n14 rue Victor Hugo 60200 Compiègne\\\\r\\\\nCedex 12 CS 10202\\\\r\\\\nTel : 03.18.38.37.37\\\\r\\\\nEmail : vallislachance@monwax.com\\\\r\\\\nMonwax \\\\r\\\\nmonwax.com\\\\r\\\\nFacebook : https://www.facebook.com/vallislachance']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁Faust', 'in', '▁Cha', 'bot', '\\\\', 'r', '\\\\', 'n', 'Ad', 'resse', '▁:', '▁19', '▁rue', '▁Descartes', '▁94', '370', '▁Suc', 'y', '-', 'en', '-', 'B', 'rie', '▁(', 'France', ')', '\\\\', 'r', '\\\\', 'n', 'Ce', 'dex', '▁9', '▁CS', '▁124', '68', '\\\\', 'r', '\\\\', 'n', 'D', 'ata', '▁Engine', 'er', '▁/', '▁Al', 'gor', 'ith', 'm', '▁X', 'Z', '▁Project', '\\\\', 'r', '\\\\', 'n', 'fa', 'ustin', 'cha', 'bot', '@', 'tel', 'e', 'w', 'orm', '.', 'com', '▁/', '▁Tel', '▁:', '▁+', '33', '▁01', '34', '35', '49', '19', '\\\\', 'r', '\\\\', 'n', 'L', 'ink', 'e', 'din', '▁:', '▁https', '://', 'fr', '.', 'link', 'e', 'din', '.', 'com', '/', 'in', '/', 'fa', 'uch', 'ab', '\\\\', 'r', '\\\\', 'n', 'T', 'ele', 'w', 'orm', '▁France', '\\\\', 'r', '\\\\', 'nt', 'ele', 'w', 'orm', '.', 'france', '.', 'com', '</s>']\n",
      "['<s>', '▁Val', 'lis', '▁La', 'ch', 'ance', '\\\\', 'r', '\\\\', 'n', 'Con', 'cept', 'eur', '▁de', '▁publications', '▁web', '▁-', '▁Un', '▁Site', ',', '▁une', '▁BD', '\\\\', 'r', '\\\\', 'n', '14', '▁rue', '▁Victor', '▁Hugo', '▁60', '200', '▁Compiègne', '\\\\', 'r', '\\\\', 'n', 'Ce', 'dex', '▁12', '▁CS', '▁102', '02', '\\\\', 'r', '\\\\', 'n', 'T', 'el', '▁:', '▁03', '.', '18', '.', '38', '.', '37', '.', '37', '\\\\', 'r', '\\\\', 'n', 'E', 'mail', '▁:', '▁val', 'lis', 'la', 'ch', 'ance', '@', 'mon', 'wa', 'x', '.', 'com', '\\\\', 'r', '\\\\', 'n', 'Mon', 'wa', 'x', '▁\\\\', 'r', '\\\\', 'n', 'mon', 'wa', 'x', '.', 'com', '\\\\', 'r', '\\\\', 'n', 'F', 'ace', 'book', '▁:', '▁https', '://', 'www', '.', 'facebook', '.', 'com', '/', 'val', 'lis', 'la', 'ch', 'ance', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizerFast\n",
    "\n",
    "MAX_LINE = 1000000\n",
    "CACHE_DIR = path.join('..', '.cache')\n",
    "TRANSFORMERS_DIR = path.join(CACHE_DIR, 'transformers')\n",
    "\n",
    "\n",
    "texts = [d['text'] for d in data['jdr'][:MAX_LINE]]\n",
    "print('Testing text:')\n",
    "display(texts[:2])\n",
    "\n",
    "#fast_tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\", cache_dir=CACHE_DIR, additional_special_tokens=['\\\\n', '\\\\r', 'https://'])\n",
    "fast_tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\", cache_dir=TRANSFORMERS_DIR)\n",
    "\n",
    "token_encodings = fast_tokenizer(texts, return_offsets_mapping=True)\n",
    "\n",
    "token_strings = [fast_tokenizer.convert_ids_to_tokens(input_ids) for input_ids in token_encodings.input_ids]\n",
    "\n",
    "for tkstr in token_strings[:2]:\n",
    "    print(tkstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66954b44-43bb-4a1c-844d-6742f3214015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2label = ['I-Reference_CS', 'B-Reference_Code_Postal', 'I-Project', 'I-Reference_Code_Postal', 'I-Human', 'B-Human', 'I-Reference_CEDEX', 'I-Email', 'I-Social_Network', 'I-Location', 'B-Reference_CEDEX', 'B-Reference_CS', 'B-Email', 'B-Phone_Number', 'B-Social_Network', 'I-Url', 'I-Organization', 'I-Phone_Number', 'B-Location', 'B-Organization', 'B-Url', 'B-Reference_User', 'I-Function', 'I-Reference_User', 'B-Project', 'B-Function', 'O']\n",
      "label2id = {'I-Reference_CS': 0, 'B-Reference_Code_Postal': 1, 'I-Project': 2, 'I-Reference_Code_Postal': 3, 'I-Human': 4, 'B-Human': 5, 'I-Reference_CEDEX': 6, 'I-Email': 7, 'I-Social_Network': 8, 'I-Location': 9, 'B-Reference_CEDEX': 10, 'B-Reference_CS': 11, 'B-Email': 12, 'B-Phone_Number': 13, 'B-Social_Network': 14, 'I-Url': 15, 'I-Organization': 16, 'I-Phone_Number': 17, 'B-Location': 18, 'B-Organization': 19, 'B-Url': 20, 'B-Reference_User': 21, 'I-Function': 22, 'I-Reference_User': 23, 'B-Project': 24, 'B-Function': 25, 'O': 26}\n"
     ]
    }
   ],
   "source": [
    "def mapping_label_token(token_span_batch, annotations_batch):\n",
    "    \"\"\"\n",
    "    Remap IOB tag to each token generated by tokenizer. Should provide the span (begin/end)\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = list()\n",
    "    \n",
    "    for token_span_sent, annotations in zip(token_span_batch, annotations_batch):\n",
    "        \n",
    "        annotations = annotations.copy()\n",
    "        entity = annotations.pop(0)\n",
    "        \n",
    "        last_label = 'O'\n",
    "        token_label = list()\n",
    "        \n",
    "        for token in token_span_sent:\n",
    "            \n",
    "            while entity['end'] < token['begin']: entity = annotations.pop(0)\n",
    "\n",
    "            if token['begin'] == token['end']:\n",
    "                label = 'O'    \n",
    "            elif entity['begin'] <= token['begin'] and token['end'] <= entity['end']:\n",
    "                prefix = 'B-' if last_label == 'O' or last_label[2:] != entity['label'] else 'I-'\n",
    "                label = prefix + entity['label']\n",
    "            else:\n",
    "                label = 'O'\n",
    "                \n",
    "            token_label.append(label)\n",
    "            last_label = label\n",
    "                \n",
    "        labels.append(token_label)\n",
    "        \n",
    "    return labels\n",
    "\n",
    "def tokenize_text(texts, annotations, tokenizer):\n",
    "    \n",
    "    # Tokenize text\n",
    "    token_encodings = tokenizer(texts, return_offsets_mapping=True)\n",
    "    token_encodings['tokens'] = [fast_tokenizer.convert_ids_to_tokens(input_ids) for input_ids in token_encodings.input_ids]\n",
    "    \n",
    "    # Mapping labels\n",
    "    token_span = token_encodings.offset_mapping\n",
    "    token_span_dict = [[{'begin': span[0], 'end': span[1]} for span in token_sent ] for token_sent in token_span]\n",
    "    token_encodings['ner_tags'] = mapping_label_token(token_span_dict, annotations)\n",
    "    \n",
    "    return token_encodings\n",
    "\n",
    "annotations = [d['annotations'] for d in data['jdr'][:MAX_LINE]]\n",
    "texts = [d['text'] for d in data['jdr'][:MAX_LINE]]\n",
    "tokenized = tokenize_text(texts, annotations, fast_tokenizer)\n",
    "\n",
    "all_labels = [i for l in tokenized['ner_tags'] for i in l ]\n",
    "unique_label = set(all_labels)\n",
    "id2label = list(unique_label)\n",
    "print('id2label =',id2label)\n",
    "label2id = {label: idx for idx, label in enumerate(id2label)}\n",
    "print('label2id =',label2id)\n",
    "\n",
    "tokenized['labels'] = [[label2id[label] for label in label_sentence] for label_sentence in tokenized['ner_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d953f3f-9020-44eb-a69a-f7a77eb41359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "id2label = list(unique_label)\n",
    "def fn_sort(x):\n",
    "    val = 0 if len(x) == 1 else (ord(x[2]))*1e3 + ord(x[0])\n",
    "    return val\n",
    "\n",
    "id2label = sorted(id2label, key=fn_sort, reverse=False)\n",
    "label2id = {label: idx for idx, label in enumerate(id2label)}\n",
    "label2id\n",
    "\n",
    "with open(path.join(CACHE_DIR, 'label_idx.json'), \"w\") as f:\n",
    "    json.dump({\n",
    "        'label2id': label2id,\n",
    "        'id2label': id2label\n",
    "    }, f, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d18a9ff0-3b91-47fc-a341-44dd5e3483b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>offset_mapping</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 28119, 236, 2614, 8674, 3155, 81, 3155, 25...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 5), (5, 7), (8, 11), (11, 14), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Faust, in, ▁Cha, bot, \\, r, \\, n, Ad, r...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[18, 3, 7, 7, 7, 18, 18, 18, 18, 18, 18, 18, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5, 1598, 4026, 61, 751, 1269, 3155, 81, 3155,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 3), (3, 6), (7, 9), (9, 11), (11,...</td>\n",
       "      <td>[&lt;s&gt;, ▁Val, lis, ▁La, ch, ance, \\, r, \\, n, Co...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, I-Huma...</td>\n",
       "      <td>[18, 3, 7, 7, 7, 7, 18, 18, 18, 18, 11, 8, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5, 11904, 73, 6445, 276, 8348, 88, 3155, 81, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 4), (4, 6), (6, 8), (8, 10), (11,...</td>\n",
       "      <td>[&lt;s&gt;, ▁Arch, ai, mb, au, ▁Mass, on, \\, r, \\, n...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, I-Huma...</td>\n",
       "      <td>[18, 3, 7, 7, 7, 7, 7, 18, 18, 18, 18, 11, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5, 470, 1606, 9313, 2265, 3155, 81, 3155, 255...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 4), (4, 8), (9, 12), (12, 16), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Jean, ette, ▁Fre, mont, \\, r, \\, n, 8, ...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[18, 3, 7, 7, 7, 18, 18, 18, 18, 2, 10, 10, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5, 3696, 19483, 236, 3155, 81, 3155, 255, 137...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 4), (5, 8), (8, 10), (10, 11), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Cher, ▁Baz, in, \\, r, \\, n, Mé, can, ic...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, O, O, O, O, B-F...</td>\n",
       "      <td>[18, 3, 7, 7, 18, 18, 18, 18, 11, 8, 8, 8, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>[5, 4114, 61, 29807, 3155, 255, 6179, 7148, 43...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 6), (7, 9), (9, 13), (13, 14), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Claude, ▁La, ndry, \\, n, Ad, resse, ▁:,...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, O, O, O, O, O, ...</td>\n",
       "      <td>[18, 3, 7, 7, 18, 18, 18, 18, 18, 2, 10, 10, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>[5, 11853, 9625, 10, 1981, 3155, 255, 3853, 30...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 9), (10, 13), (13, 14), (14, 18),...</td>\n",
       "      <td>[&lt;s&gt;, ▁Charlotte, ▁Bus, s, ière, \\, n, 59, ▁co...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[18, 3, 7, 7, 7, 18, 18, 2, 10, 10, 10, 10, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>[5, 18467, 24817, 3155, 255, 3225, 9, 3220, 9,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 6), (7, 14), (14, 15), (15, 16), ...</td>\n",
       "      <td>[&lt;s&gt;, ▁Cédric, ▁Garnier, \\, n, 04, ., 27, ., 1...</td>\n",
       "      <td>[O, B-Human, I-Human, O, O, B-Phone_Number, I-...</td>\n",
       "      <td>[18, 3, 7, 18, 18, 16, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>[5, 14147, 10223, 11734, 4461, 3155, 255, 3395...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 3), (3, 9), (10, 13), (13, 16), (...</td>\n",
       "      <td>[&lt;s&gt;, ▁Fla, vienne, ▁Dev, ost, \\, n, 02, ., 56...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[18, 3, 7, 7, 7, 18, 18, 16, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>[5, 19937, 487, 10340, 2872, 3155, 255, 585, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 6), (6, 8), (9, 12), (12, 16), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Violet, te, ▁Fau, bert, \\, n, O, m, ni,...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[18, 3, 7, 7, 7, 18, 18, 17, 20, 20, 20, 20, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>473 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_ids  \\\n",
       "0    [5, 28119, 236, 2614, 8674, 3155, 81, 3155, 25...   \n",
       "1    [5, 1598, 4026, 61, 751, 1269, 3155, 81, 3155,...   \n",
       "2    [5, 11904, 73, 6445, 276, 8348, 88, 3155, 81, ...   \n",
       "3    [5, 470, 1606, 9313, 2265, 3155, 81, 3155, 255...   \n",
       "4    [5, 3696, 19483, 236, 3155, 81, 3155, 255, 137...   \n",
       "..                                                 ...   \n",
       "468  [5, 4114, 61, 29807, 3155, 255, 6179, 7148, 43...   \n",
       "469  [5, 11853, 9625, 10, 1981, 3155, 255, 3853, 30...   \n",
       "470  [5, 18467, 24817, 3155, 255, 3225, 9, 3220, 9,...   \n",
       "471  [5, 14147, 10223, 11734, 4461, 3155, 255, 3395...   \n",
       "472  [5, 19937, 487, 10340, 2872, 3155, 255, 585, 2...   \n",
       "\n",
       "                                        attention_mask  \\\n",
       "0    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "..                                                 ...   \n",
       "468  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "469  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "470  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "471  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "472  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                        offset_mapping  \\\n",
       "0    [(0, 0), (0, 5), (5, 7), (8, 11), (11, 14), (1...   \n",
       "1    [(0, 0), (0, 3), (3, 6), (7, 9), (9, 11), (11,...   \n",
       "2    [(0, 0), (0, 4), (4, 6), (6, 8), (8, 10), (11,...   \n",
       "3    [(0, 0), (0, 4), (4, 8), (9, 12), (12, 16), (1...   \n",
       "4    [(0, 0), (0, 4), (5, 8), (8, 10), (10, 11), (1...   \n",
       "..                                                 ...   \n",
       "468  [(0, 0), (0, 6), (7, 9), (9, 13), (13, 14), (1...   \n",
       "469  [(0, 0), (0, 9), (10, 13), (13, 14), (14, 18),...   \n",
       "470  [(0, 0), (0, 6), (7, 14), (14, 15), (15, 16), ...   \n",
       "471  [(0, 0), (0, 3), (3, 9), (10, 13), (13, 16), (...   \n",
       "472  [(0, 0), (0, 6), (6, 8), (9, 12), (12, 16), (1...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [<s>, ▁Faust, in, ▁Cha, bot, \\, r, \\, n, Ad, r...   \n",
       "1    [<s>, ▁Val, lis, ▁La, ch, ance, \\, r, \\, n, Co...   \n",
       "2    [<s>, ▁Arch, ai, mb, au, ▁Mass, on, \\, r, \\, n...   \n",
       "3    [<s>, ▁Jean, ette, ▁Fre, mont, \\, r, \\, n, 8, ...   \n",
       "4    [<s>, ▁Cher, ▁Baz, in, \\, r, \\, n, Mé, can, ic...   \n",
       "..                                                 ...   \n",
       "468  [<s>, ▁Claude, ▁La, ndry, \\, n, Ad, resse, ▁:,...   \n",
       "469  [<s>, ▁Charlotte, ▁Bus, s, ière, \\, n, 59, ▁co...   \n",
       "470  [<s>, ▁Cédric, ▁Garnier, \\, n, 04, ., 27, ., 1...   \n",
       "471  [<s>, ▁Fla, vienne, ▁Dev, ost, \\, n, 02, ., 56...   \n",
       "472  [<s>, ▁Violet, te, ▁Fau, bert, \\, n, O, m, ni,...   \n",
       "\n",
       "                                              ner_tags  \\\n",
       "0    [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "1    [O, B-Human, I-Human, I-Human, I-Human, I-Huma...   \n",
       "2    [O, B-Human, I-Human, I-Human, I-Human, I-Huma...   \n",
       "3    [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "4    [O, B-Human, I-Human, I-Human, O, O, O, O, B-F...   \n",
       "..                                                 ...   \n",
       "468  [O, B-Human, I-Human, I-Human, O, O, O, O, O, ...   \n",
       "469  [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "470  [O, B-Human, I-Human, O, O, B-Phone_Number, I-...   \n",
       "471  [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "472  [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "\n",
       "                                                labels  \n",
       "0    [18, 3, 7, 7, 7, 18, 18, 18, 18, 18, 18, 18, 2...  \n",
       "1    [18, 3, 7, 7, 7, 7, 18, 18, 18, 18, 11, 8, 8, ...  \n",
       "2    [18, 3, 7, 7, 7, 7, 7, 18, 18, 18, 18, 11, 8, ...  \n",
       "3    [18, 3, 7, 7, 7, 18, 18, 18, 18, 2, 10, 10, 10...  \n",
       "4    [18, 3, 7, 7, 18, 18, 18, 18, 11, 8, 8, 8, 8, ...  \n",
       "..                                                 ...  \n",
       "468  [18, 3, 7, 7, 18, 18, 18, 18, 18, 2, 10, 10, 1...  \n",
       "469  [18, 3, 7, 7, 7, 18, 18, 2, 10, 10, 10, 10, 12...  \n",
       "470  [18, 3, 7, 18, 18, 16, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "471  [18, 3, 7, 7, 7, 18, 18, 16, 0, 0, 0, 0, 0, 0,...  \n",
       "472  [18, 3, 7, 7, 7, 18, 18, 17, 20, 20, 20, 20, 1...  \n",
       "\n",
       "[473 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dict(tokenized))\n",
    "os.makedirs(path.join('..', '.cache'), exist_ok=True)\n",
    "df.to_parquet(path.join('..','.cache', 'jdr.parquet'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81fafc63-f55f-4927-9a4e-f68dc4e990c3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wt/74qzbgts2pjdbwt397c_q5dr00m9p0/T/ipykernel_28109/514417744.py:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForTokenClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "EarlyStoppingCallback\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 473\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 600\n",
      "  Number of trainable parameters = 110052123\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 13/600 01:30 < 1:20:50, 0.12 it/s, Epoch 0.40/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 85\u001b[0m\n\u001b[1;32m     60\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     61\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,  \u001b[38;5;66;03m# batch size per device during training\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,   \u001b[38;5;66;03m# batch size for evaluation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     71\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     72\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     73\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     ]\n\u001b[1;32m     83\u001b[0m )\n\u001b[0;32m---> 85\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/transformers/trainer.py:2526\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2526\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForTokenClassification, EarlyStoppingCallback\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextMineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.data = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self): raise IndexError  # meet the end of dataset\n",
    "        sample = self.data.loc[idx].to_dict()\n",
    "        #for k, v in sample.items():\n",
    "        #   print(k, '=', len(v))\n",
    "        return {'input_ids': sample['input_ids'], 'attention_mask': sample['attention_mask'],'labels': sample['labels']}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "dataset = TextMineDataset(df)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "        if(k not in flattened_results.keys()):\n",
    "            flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"camembert-base\", num_labels=len(id2label), id2label=id2label, label2id=label2id, cache_dir=CACHE_DIR)\n",
    "data_collator = DataCollatorForTokenClassification(fast_tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    num_train_epochs=20,\n",
    "    output_dir=path.join('..', '.cache', 'results'),\n",
    "    logging_dir=path.join('..', '.cache', 'logs'),\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=fast_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "        TensorBoardCallback(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db80e4-e697-4350-8ef1-0e4928037170",
   "metadata": {},
   "source": [
    "## Make Pytorch TextMine Dataset from raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5153edbf-c8fa-469e-a2d9-6e2db1567582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cache data from ../.cache/jdr.parquet\n",
      "Load label idx from ../.cache/label_idx.json\n",
      "Load cache data from ../.cache/jdf.parquet\n",
      "Load label idx from ../.cache/label_idx.json\n"
     ]
    }
   ],
   "source": [
    "from data.textmine import TextMineDataset\n",
    "from transformers import CamembertTokenizerFast\n",
    "\n",
    "DATA_PATH = path.join('..', 'dataset')\n",
    "CACHE_DIR = path.join('..', '.cache')\n",
    "\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\", cache_dir=path.join(CACHE_DIR, 'transformers'))\n",
    "jdr = TextMineDataset('jdr', tokenizer=tokenizer, data_path=DATA_PATH, cache=CACHE_DIR)\n",
    "jdf = TextMineDataset('jdf', tokenizer=tokenizer, data_path=DATA_PATH, cache=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f277608-42b6-49bc-87f0-39f7ab9088ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForTokenClassification, EarlyStoppingCallback\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from data.textmine import TextMineDataset\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p): \n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [ [jdf.id2label[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ]\n",
    "    true_labels = [ [jdf.id2label[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=-1.)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "        if(k not in flattened_results.keys()):\n",
    "            flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"camembert-base\", num_labels=len(jdf.id2label), id2label=jdf.id2label, label2id=jdf.label2id, cache_dir=CACHE_DIR)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    num_train_epochs=20,\n",
    "    output_dir=path.join('..', '.cache', 'results', 'train_jdf_val_jdr'),\n",
    "    logging_dir=path.join('..', '.cache', 'logs', 'train_jdf_val_jdr'),\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055be41f-c9eb-4173-8085-7301887485df",
   "metadata": {},
   "source": [
    "Train on `jdr`, val on `jdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a5487c0-3a26-4e04-93c7-d025c709e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JDF:\n",
      "idx_labels: {0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 14, 15, 20}\n",
      "str_labels: ['O', 'B-Email', 'I-Email', 'B-Human', 'I-Human', 'B-Location', 'I-Location', 'B-Organization', 'I-Organization', 'B-Phone_Number', 'I-Phone_Number', 'B-Reference_Code_Postal', 'I-Reference_Code_Postal']\n"
     ]
    }
   ],
   "source": [
    "print('JDF:')\n",
    "labels = [set(l) for l in jdf.data.labels]\n",
    "labels = [l for l_array in labels for l in l_array]\n",
    "labels = set(labels)\n",
    "print('idx_labels:', labels)\n",
    "print('str_labels:', [jdf.id2label[l] for l in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0c89d5c-fda5-4bcc-8f1c-dbdce3be7988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JDR:\n",
      "idx_labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}\n",
      "str_labels: ['O', 'B-Email', 'I-Email', 'B-Function', 'I-Function', 'B-Human', 'I-Human', 'B-Location', 'I-Location', 'B-Organization', 'I-Organization', 'B-Phone_Number', 'B-Project', 'I-Project', 'I-Phone_Number', 'B-Reference_Code_Postal', 'B-Reference_CEDEX', 'B-Reference_CS', 'B-Reference_User', 'I-Reference_CS', 'I-Reference_Code_Postal', 'I-Reference_CEDEX', 'I-Reference_User', 'B-Social_Network', 'I-Social_Network', 'B-Url', 'I-Url']\n"
     ]
    }
   ],
   "source": [
    "print('JDR:')\n",
    "labels = [set(l) for l in jdr.data.labels]\n",
    "labels = [l for l_array in labels for l in l_array]\n",
    "labels = set(labels)\n",
    "print('idx_labels:', labels)\n",
    "print('str_labels:', [jdr.id2label[l] for l in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "79fbee2a-6bea-47a2-9a87-937e1e07a3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 640\n",
      "  Number of trainable parameters = 110052123\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [576/640 1:02:03 < 06:55, 0.15 it/s, Epoch 18/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Email F1</th>\n",
       "      <th>Function F1</th>\n",
       "      <th>Human F1</th>\n",
       "      <th>Location F1</th>\n",
       "      <th>Organization F1</th>\n",
       "      <th>Phone Number F1</th>\n",
       "      <th>Project F1</th>\n",
       "      <th>Reference Cedex F1</th>\n",
       "      <th>Reference Cs F1</th>\n",
       "      <th>Reference Code Postal F1</th>\n",
       "      <th>Reference User F1</th>\n",
       "      <th>Social Network F1</th>\n",
       "      <th>Url F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.096800</td>\n",
       "      <td>2.249593</td>\n",
       "      <td>0.455177</td>\n",
       "      <td>0.389309</td>\n",
       "      <td>0.419674</td>\n",
       "      <td>0.648971</td>\n",
       "      <td>0.200466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.905847</td>\n",
       "      <td>0.609732</td>\n",
       "      <td>0.023064</td>\n",
       "      <td>0.477642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.547200</td>\n",
       "      <td>1.808935</td>\n",
       "      <td>0.599062</td>\n",
       "      <td>0.620410</td>\n",
       "      <td>0.609549</td>\n",
       "      <td>0.772674</td>\n",
       "      <td>0.377850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.889324</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.381620</td>\n",
       "      <td>0.618474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.282400</td>\n",
       "      <td>1.631734</td>\n",
       "      <td>0.572599</td>\n",
       "      <td>0.603672</td>\n",
       "      <td>0.587725</td>\n",
       "      <td>0.784500</td>\n",
       "      <td>0.350679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.865878</td>\n",
       "      <td>0.785948</td>\n",
       "      <td>0.434583</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.103000</td>\n",
       "      <td>1.503148</td>\n",
       "      <td>0.539953</td>\n",
       "      <td>0.623920</td>\n",
       "      <td>0.578908</td>\n",
       "      <td>0.792828</td>\n",
       "      <td>0.346112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.881592</td>\n",
       "      <td>0.790226</td>\n",
       "      <td>0.362292</td>\n",
       "      <td>0.563654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.965400</td>\n",
       "      <td>1.381335</td>\n",
       "      <td>0.585372</td>\n",
       "      <td>0.646058</td>\n",
       "      <td>0.614220</td>\n",
       "      <td>0.805681</td>\n",
       "      <td>0.424173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.873767</td>\n",
       "      <td>0.809204</td>\n",
       "      <td>0.434048</td>\n",
       "      <td>0.614341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.940559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.877900</td>\n",
       "      <td>1.325044</td>\n",
       "      <td>0.549306</td>\n",
       "      <td>0.630130</td>\n",
       "      <td>0.586948</td>\n",
       "      <td>0.796479</td>\n",
       "      <td>0.397714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859406</td>\n",
       "      <td>0.794159</td>\n",
       "      <td>0.383595</td>\n",
       "      <td>0.579512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.852600</td>\n",
       "      <td>1.248609</td>\n",
       "      <td>0.568321</td>\n",
       "      <td>0.627700</td>\n",
       "      <td>0.596536</td>\n",
       "      <td>0.801612</td>\n",
       "      <td>0.416953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.865135</td>\n",
       "      <td>0.781991</td>\n",
       "      <td>0.397653</td>\n",
       "      <td>0.608016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.924399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.665400</td>\n",
       "      <td>1.181911</td>\n",
       "      <td>0.542783</td>\n",
       "      <td>0.618251</td>\n",
       "      <td>0.578064</td>\n",
       "      <td>0.806518</td>\n",
       "      <td>0.416465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.918750</td>\n",
       "      <td>0.789174</td>\n",
       "      <td>0.298524</td>\n",
       "      <td>0.616290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.930796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.650300</td>\n",
       "      <td>1.152512</td>\n",
       "      <td>0.529952</td>\n",
       "      <td>0.623380</td>\n",
       "      <td>0.572882</td>\n",
       "      <td>0.800966</td>\n",
       "      <td>0.419394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.867420</td>\n",
       "      <td>0.788973</td>\n",
       "      <td>0.314903</td>\n",
       "      <td>0.624506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.934028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.579400</td>\n",
       "      <td>1.074131</td>\n",
       "      <td>0.598232</td>\n",
       "      <td>0.657667</td>\n",
       "      <td>0.626543</td>\n",
       "      <td>0.816785</td>\n",
       "      <td>0.457831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.896130</td>\n",
       "      <td>0.802804</td>\n",
       "      <td>0.376187</td>\n",
       "      <td>0.761715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.940351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.585600</td>\n",
       "      <td>1.052633</td>\n",
       "      <td>0.592864</td>\n",
       "      <td>0.654968</td>\n",
       "      <td>0.622370</td>\n",
       "      <td>0.814846</td>\n",
       "      <td>0.453237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.887984</td>\n",
       "      <td>0.825520</td>\n",
       "      <td>0.375152</td>\n",
       "      <td>0.733202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.916239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>1.041212</td>\n",
       "      <td>0.581598</td>\n",
       "      <td>0.648488</td>\n",
       "      <td>0.613224</td>\n",
       "      <td>0.811728</td>\n",
       "      <td>0.434164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.877510</td>\n",
       "      <td>0.814959</td>\n",
       "      <td>0.363528</td>\n",
       "      <td>0.733399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.928943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.503300</td>\n",
       "      <td>1.035646</td>\n",
       "      <td>0.558482</td>\n",
       "      <td>0.635529</td>\n",
       "      <td>0.594520</td>\n",
       "      <td>0.807811</td>\n",
       "      <td>0.425030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870871</td>\n",
       "      <td>0.816109</td>\n",
       "      <td>0.310624</td>\n",
       "      <td>0.706575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.913413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.414300</td>\n",
       "      <td>0.989679</td>\n",
       "      <td>0.614535</td>\n",
       "      <td>0.675756</td>\n",
       "      <td>0.643693</td>\n",
       "      <td>0.819561</td>\n",
       "      <td>0.480287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880642</td>\n",
       "      <td>0.828544</td>\n",
       "      <td>0.421984</td>\n",
       "      <td>0.782953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.910321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.415300</td>\n",
       "      <td>0.975490</td>\n",
       "      <td>0.627266</td>\n",
       "      <td>0.681965</td>\n",
       "      <td>0.653473</td>\n",
       "      <td>0.821310</td>\n",
       "      <td>0.487981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.885974</td>\n",
       "      <td>0.832054</td>\n",
       "      <td>0.445013</td>\n",
       "      <td>0.792864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>0.980832</td>\n",
       "      <td>0.613363</td>\n",
       "      <td>0.674136</td>\n",
       "      <td>0.642315</td>\n",
       "      <td>0.817318</td>\n",
       "      <td>0.486161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.883065</td>\n",
       "      <td>0.832758</td>\n",
       "      <td>0.415366</td>\n",
       "      <td>0.785010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.390800</td>\n",
       "      <td>0.978354</td>\n",
       "      <td>0.612401</td>\n",
       "      <td>0.669276</td>\n",
       "      <td>0.639577</td>\n",
       "      <td>0.816367</td>\n",
       "      <td>0.475390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882591</td>\n",
       "      <td>0.832274</td>\n",
       "      <td>0.423906</td>\n",
       "      <td>0.749263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.937282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.975630</td>\n",
       "      <td>0.611771</td>\n",
       "      <td>0.667927</td>\n",
       "      <td>0.638616</td>\n",
       "      <td>0.815911</td>\n",
       "      <td>0.472422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.881459</td>\n",
       "      <td>0.829529</td>\n",
       "      <td>0.423602</td>\n",
       "      <td>0.746298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-32\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-32/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-32/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-64\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-64/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-96\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-96/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-128\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-128/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-128/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-160\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-160/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-160/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-192\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-192/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-224\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-224/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-224/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-224/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-224/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-256\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-256/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-256/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-288\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-288/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-320\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-320/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-320/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-352\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-352/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-352/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-352/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-352/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-384\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-384/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-384/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-384/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-384/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-416\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-416/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-416/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-416/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-416/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-448\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-448/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-448/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-448/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-448/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-480\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-480/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-480/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-480/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-480/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-512\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-512/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-512/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-512/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-512/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-544\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-544/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-544/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-544/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-544/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-576\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-576/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-576/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-576/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-576/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../.cache/results/train_jdf_val_jdr/checkpoint-480 (score: 0.9754904508590698).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=576, training_loss=0.8499794393881328, metrics={'train_runtime': 3730.2654, 'train_samples_per_second': 2.681, 'train_steps_per_second': 0.172, 'total_flos': 241727585790744.0, 'train_loss': 0.8499794393881328, 'epoch': 18.0})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=jdf,\n",
    "    eval_dataset=jdr,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4d708-1d3a-40a0-aca9-06ed339c68da",
   "metadata": {},
   "source": [
    "## Reshuffle JDR and JDF, get new splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e2e1e03-7a85-4ad6-a955-5fb5cb41b0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cache data from ../.cache/train.parquet\n",
      "Load label idx from ../.cache/label_idx.json\n",
      "Load cache data from ../.cache/val.parquet\n",
      "Load label idx from ../.cache/label_idx.json\n"
     ]
    }
   ],
   "source": [
    "# load jdf and jdr\n",
    "df_jdf = pd.read_parquet(path.join(CACHE_DIR, 'jdf.parquet'))\n",
    "df_jdr = pd.read_parquet(path.join(CACHE_DIR, 'jdr.parquet'))\n",
    "df_jdr = df_jdr.drop(columns=['labels'])\n",
    "\n",
    "# fusion into full data\n",
    "full_data = pd.concat([df_jdf, df_jdr], ignore_index=True)\n",
    "for col in full_data.columns:\n",
    "    if isinstance(full_data.loc[0, col], np.ndarray):\n",
    "        full_data[col] = full_data[col].apply(lambda x: x.tolist())\n",
    "full_data.to_parquet(path.join(CACHE_DIR, 'full.parquet'))\n",
    "\n",
    "# split and generate dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "train, val = train_test_split(full_data, test_size=.15)\n",
    "train.to_parquet(path.join(CACHE_DIR, 'train.parquet'), index=False)\n",
    "val.to_parquet(path.join(CACHE_DIR, 'val.parquet'), index=False)\n",
    "\n",
    "trainset = TextMineDataset('train', tokenizer=tokenizer, data_path=DATA_PATH, cache=CACHE_DIR)\n",
    "valset = TextMineDataset('val', tokenizer=tokenizer, data_path=DATA_PATH, cache=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457da7df-2a42-4a0c-a1fd-6af74d09bc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 827\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1040\n",
      "  Number of trainable parameters = 110052123\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 782/1040 1:45:30 < 34:54, 0.12 it/s, Epoch 15.02/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Email F1</th>\n",
       "      <th>Function F1</th>\n",
       "      <th>Human F1</th>\n",
       "      <th>Location F1</th>\n",
       "      <th>Organization F1</th>\n",
       "      <th>Phone Number F1</th>\n",
       "      <th>Project F1</th>\n",
       "      <th>Reference Cedex F1</th>\n",
       "      <th>Reference Cs F1</th>\n",
       "      <th>Reference Code Postal F1</th>\n",
       "      <th>Reference User F1</th>\n",
       "      <th>Social Network F1</th>\n",
       "      <th>Url F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.741700</td>\n",
       "      <td>1.658992</td>\n",
       "      <td>0.908745</td>\n",
       "      <td>0.918348</td>\n",
       "      <td>0.913521</td>\n",
       "      <td>0.891095</td>\n",
       "      <td>0.953271</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>0.962233</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.928302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.867470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.302000</td>\n",
       "      <td>1.210943</td>\n",
       "      <td>0.945817</td>\n",
       "      <td>0.955812</td>\n",
       "      <td>0.950788</td>\n",
       "      <td>0.963212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>0.970492</td>\n",
       "      <td>0.907143</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.947867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.930233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.046500</td>\n",
       "      <td>0.960262</td>\n",
       "      <td>0.946869</td>\n",
       "      <td>0.958694</td>\n",
       "      <td>0.952745</td>\n",
       "      <td>0.968613</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>0.983498</td>\n",
       "      <td>0.921986</td>\n",
       "      <td>0.979920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.787900</td>\n",
       "      <td>0.771485</td>\n",
       "      <td>0.960615</td>\n",
       "      <td>0.960615</td>\n",
       "      <td>0.960615</td>\n",
       "      <td>0.981606</td>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.912621</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>0.985124</td>\n",
       "      <td>0.943262</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.649700</td>\n",
       "      <td>0.626723</td>\n",
       "      <td>0.954286</td>\n",
       "      <td>0.962536</td>\n",
       "      <td>0.958393</td>\n",
       "      <td>0.985109</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.946996</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.580700</td>\n",
       "      <td>0.517963</td>\n",
       "      <td>0.955238</td>\n",
       "      <td>0.963497</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.985693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.989831</td>\n",
       "      <td>0.983498</td>\n",
       "      <td>0.939929</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.964706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.434400</td>\n",
       "      <td>0.434567</td>\n",
       "      <td>0.964695</td>\n",
       "      <td>0.971182</td>\n",
       "      <td>0.967927</td>\n",
       "      <td>0.986277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.989831</td>\n",
       "      <td>0.988430</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.964706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.358200</td>\n",
       "      <td>0.372642</td>\n",
       "      <td>0.965682</td>\n",
       "      <td>0.973103</td>\n",
       "      <td>0.969378</td>\n",
       "      <td>0.987299</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.989831</td>\n",
       "      <td>0.988430</td>\n",
       "      <td>0.942446</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.411300</td>\n",
       "      <td>0.328010</td>\n",
       "      <td>0.964829</td>\n",
       "      <td>0.975024</td>\n",
       "      <td>0.969900</td>\n",
       "      <td>0.988029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>0.988430</td>\n",
       "      <td>0.960289</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.270600</td>\n",
       "      <td>0.295367</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975024</td>\n",
       "      <td>0.970827</td>\n",
       "      <td>0.989489</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.979730</td>\n",
       "      <td>0.986799</td>\n",
       "      <td>0.952727</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.286900</td>\n",
       "      <td>0.268238</td>\n",
       "      <td>0.966635</td>\n",
       "      <td>0.974063</td>\n",
       "      <td>0.970335</td>\n",
       "      <td>0.989781</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>0.985124</td>\n",
       "      <td>0.945848</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.231700</td>\n",
       "      <td>0.249318</td>\n",
       "      <td>0.975120</td>\n",
       "      <td>0.978866</td>\n",
       "      <td>0.976989</td>\n",
       "      <td>0.990657</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>0.988430</td>\n",
       "      <td>0.967273</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.976190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.189000</td>\n",
       "      <td>0.234522</td>\n",
       "      <td>0.976008</td>\n",
       "      <td>0.976945</td>\n",
       "      <td>0.976476</td>\n",
       "      <td>0.989343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>0.988430</td>\n",
       "      <td>0.948905</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.192400</td>\n",
       "      <td>0.219508</td>\n",
       "      <td>0.975143</td>\n",
       "      <td>0.979827</td>\n",
       "      <td>0.977480</td>\n",
       "      <td>0.990949</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>0.985173</td>\n",
       "      <td>0.967273</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>0.210775</td>\n",
       "      <td>0.976077</td>\n",
       "      <td>0.979827</td>\n",
       "      <td>0.977948</td>\n",
       "      <td>0.990657</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>0.988430</td>\n",
       "      <td>0.963768</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-52\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-52/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-52/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-52/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-52/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-104\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-104/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-104/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-104/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-104/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-156\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-156/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-156/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-156/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-156/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-52] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-208\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-208/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-104] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-260\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-260/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-260/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-260/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-260/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-156] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-312\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-312/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-312/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-312/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-312/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-364\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-364/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-364/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-364/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-364/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-416\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-416/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-416/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-416/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-416/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-312] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-468\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-468/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-468/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-468/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-468/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-364] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-520\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-520/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-520/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-520/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-520/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-572\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-572/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-572/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-572/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-572/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-468] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-624\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-624/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-624/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-624/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-624/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-520] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-676\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-676/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-676/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-676/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-676/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-572] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-728\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-728/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-728/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-728/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-728/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-624] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../.cache/results/train_jdf_val_jdr/checkpoint-780\n",
      "Configuration saved in ../.cache/results/train_jdf_val_jdr/checkpoint-780/config.json\n",
      "Model weights saved in ../.cache/results/train_jdf_val_jdr/checkpoint-780/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-780/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_jdf_val_jdr/checkpoint-780/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_jdf_val_jdr/checkpoint-676] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    num_train_epochs=20,\n",
    "    output_dir=path.join('..', '.cache', 'results', 'train_vaL_split'),\n",
    "    logging_dir=path.join('..', '.cache', 'logs', 'train_vaL_split'),\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=valset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25af04a-0aa8-45d9-b9f6-e8a9afaf9c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789d4e9-0719-4470-b723-145cb09f1edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a9480-b704-4687-b887-2c506a18516c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f89f59f1-0801-442c-a0a8-1cc606816627",
   "metadata": {},
   "source": [
    "Classify subset of entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243e514-dee8-441d-8294-eb8bb40768bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = ['Human', 'Project', 'Organization', 'Reference_User']\n",
    "new_spec_tokens = list(set(train.id2label) - set() )\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee435c5-fd5d-4db5-9187-85d6db514656",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Human',\n",
    " 'Location',\n",
    " 'Reference_Code_Postal',\n",
    " 'Reference_CEDEX',\n",
    " 'Reference_CS',\n",
    " 'Function',\n",
    " 'Project',\n",
    " 'Email',\n",
    " 'Phone_Number',\n",
    " 'Social_Network',\n",
    " 'Reference_User',\n",
    " 'Organization',\n",
    " 'Url'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
