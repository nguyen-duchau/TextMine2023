{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991db548-8934-44fe-9608-3a581720de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "sys.path.append(\"./../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a78f401-60b1-40d0-b8a2-94b1da80845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JDR #examples : 473\n",
      "JDF #examples : 500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Les entités nommées</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>label</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Faustin</td>\n",
       "      <td>Human</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chabot</td>\n",
       "      <td>Human</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>Location</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rue</td>\n",
       "      <td>Location</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Descartes</td>\n",
       "      <td>Location</td>\n",
       "      <td>35</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        form     label  begin  end\n",
       "0    Faustin     Human      0    7\n",
       "1     Chabot     Human      8   14\n",
       "2         19  Location     28   30\n",
       "3        rue  Location     31   34\n",
       "4  Descartes  Location     35   44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Human', 'Location', 'Reference_Code_Postal', 'Reference_CEDEX', 'Reference_CS', ..., 'Phone_Number', 'Social_Network', 'Reference_User', 'Organization', 'Url']\n",
      "Length: 13\n",
      "Categories (13, object): ['Email', 'Function', 'Human', 'Location', ..., 'Reference_Code_Postal', 'Reference_User', 'Social_Network', 'Url']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "DATA_PATH = path.join('..','dataset')\n",
    "JDF_PATH = path.join(DATA_PATH, 'JDF.json')\n",
    "JDR_PATH = path.join(DATA_PATH, 'JDR.json')\n",
    "\n",
    "data = dict()\n",
    "\n",
    "with open(JDR_PATH, 'r') as f:\n",
    "    data['jdr'] = json.load(f)\n",
    "    \n",
    "with open(JDF_PATH, 'r') as f:\n",
    "    data['jdf'] = json.load(f)\n",
    "\n",
    "print('JDR #examples :',len(data['jdr']))\n",
    "print('JDF #examples :',len(data['jdf']))\n",
    "\n",
    "annotations = [a for d in data['jdr'] for a in d['annotations']]\n",
    "df_annotations = pd.DataFrame(annotations)\n",
    "df_annotations[\"label\"] = df_annotations[\"label\"].astype(\"category\")\n",
    "display(HTML('<h3>Les entités nommées</h3>'))\n",
    "display(df_annotations.head())\n",
    "\n",
    "print('Labels:', df_annotations['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab065a9-153a-41be-8c8b-8b1a68e4dff2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Format d'une pharse donnée</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a14ce8-46de-4929-97bc-94b7dcb2dfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'identifier': 500,\n",
       " 'text': 'Faustin Chabot\\\\r\\\\nAdresse : 19 rue Descartes 94370 Sucy-en-Brie (France)\\\\r\\\\nCedex 9 CS 12468\\\\r\\\\nData Engineer / Algorithm XZ Project\\\\r\\\\nfaustinchabot@teleworm.com / Tel : +33 0134354919\\\\r\\\\nLinkedin : https://fr.linkedin.com/in/fauchab\\\\r\\\\nTeleworm France\\\\r\\\\nteleworm.france.com',\n",
       " 'annotations': [{'form': 'Faustin', 'label': 'Human', 'begin': 0, 'end': 7},\n",
       "  {'form': 'Chabot', 'label': 'Human', 'begin': 8, 'end': 14},\n",
       "  {'form': '19', 'label': 'Location', 'begin': 28, 'end': 30},\n",
       "  {'form': 'rue', 'label': 'Location', 'begin': 31, 'end': 34},\n",
       "  {'form': 'Descartes', 'label': 'Location', 'begin': 35, 'end': 44},\n",
       "  {'form': '94370', 'label': 'Reference_Code_Postal', 'begin': 45, 'end': 50},\n",
       "  {'form': 'Sucy-en-Brie', 'label': 'Location', 'begin': 51, 'end': 63},\n",
       "  {'form': 'France', 'label': 'Location', 'begin': 65, 'end': 71},\n",
       "  {'form': 'Cedex', 'label': 'Reference_CEDEX', 'begin': 76, 'end': 81},\n",
       "  {'form': '9', 'label': 'Reference_CEDEX', 'begin': 82, 'end': 83},\n",
       "  {'form': 'CS', 'label': 'Reference_CS', 'begin': 84, 'end': 86},\n",
       "  {'form': '12468', 'label': 'Reference_CS', 'begin': 87, 'end': 92},\n",
       "  {'form': 'Data', 'label': 'Function', 'begin': 96, 'end': 100},\n",
       "  {'form': 'Engineer', 'label': 'Function', 'begin': 101, 'end': 109},\n",
       "  {'form': 'Algorithm', 'label': 'Project', 'begin': 112, 'end': 121},\n",
       "  {'form': 'XZ', 'label': 'Project', 'begin': 122, 'end': 124},\n",
       "  {'form': 'Project', 'label': 'Project', 'begin': 125, 'end': 132},\n",
       "  {'form': 'faustinchabot@teleworm.com',\n",
       "   'label': 'Email',\n",
       "   'begin': 136,\n",
       "   'end': 162},\n",
       "  {'form': '+33 0134354919',\n",
       "   'label': 'Phone_Number',\n",
       "   'begin': 171,\n",
       "   'end': 185},\n",
       "  {'form': 'Linkedin', 'label': 'Social_Network', 'begin': 189, 'end': 197},\n",
       "  {'form': 'https://fr.linkedin.com/in/fauchab',\n",
       "   'label': 'Reference_User',\n",
       "   'begin': 200,\n",
       "   'end': 234},\n",
       "  {'form': 'Teleworm', 'label': 'Organization', 'begin': 238, 'end': 246},\n",
       "  {'form': 'France', 'label': 'Organization', 'begin': 247, 'end': 253},\n",
       "  {'form': 'teleworm.france.com', 'label': 'Url', 'begin': 257, 'end': 276}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['jdr'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8240750-6ed2-4bf7-affa-d81db22eae02",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Est-ce qu'il y a de motif pour le numéro de téléphone?</div>\n",
    "\n",
    "> Apparemment non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538bdd22-5edd-488f-9e6d-57c720ddb879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>label</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>+33 0134354919</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>171</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>03.18.38.37.37</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>136</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>+ 03 81 20 48 27</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>152</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>01.75.88.25.30</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>77</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>+ 33 01 77 83 74 05</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>157</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>+33 0365962110</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>70</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>01.55.29.21.75</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>118</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>+ 33 01 79 28 30 87</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>75</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>33 0147908347</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>150</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>03.54.57.86.42</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>162</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>+ 33 0529308213</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>113</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>03 46 69 07 08</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>121</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>+ 33 0221446127</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>129</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>01.92.26.82.75</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>124</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>01.17.82.71.60</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>151</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>+33 01.26.31.62.25</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>86</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>02 66 69 43 14</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>143</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>02.96.95.55.61</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>146</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>+ 33 01.47.04.39.40</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>134</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>+33 04.12.26.56.34</td>\n",
       "      <td>Phone_Number</td>\n",
       "      <td>90</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    form         label  begin  end\n",
       "18        +33 0134354919  Phone_Number    171  185\n",
       "45        03.18.38.37.37  Phone_Number    136  150\n",
       "72      + 03 81 20 48 27  Phone_Number    152  168\n",
       "91        01.75.88.25.30  Phone_Number     77   91\n",
       "122  + 33 01 77 83 74 05  Phone_Number    157  176\n",
       "134       +33 0365962110  Phone_Number     70   84\n",
       "170       01.55.29.21.75  Phone_Number    118  132\n",
       "191  + 33 01 79 28 30 87  Phone_Number     75   94\n",
       "225        33 0147908347  Phone_Number    150  163\n",
       "250       03.54.57.86.42  Phone_Number    162  176\n",
       "269      + 33 0529308213  Phone_Number    113  128\n",
       "291       03 46 69 07 08  Phone_Number    121  135\n",
       "315      + 33 0221446127  Phone_Number    129  144\n",
       "333       01.92.26.82.75  Phone_Number    124  138\n",
       "364       01.17.82.71.60  Phone_Number    151  165\n",
       "378   +33 01.26.31.62.25  Phone_Number     86  104\n",
       "418       02 66 69 43 14  Phone_Number    143  157\n",
       "443       02.96.95.55.61  Phone_Number    146  160\n",
       "465  + 33 01.47.04.39.40  Phone_Number    134  153\n",
       "482   +33 04.12.26.56.34  Phone_Number     90  108"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations[df_annotations['label']=='Phone_Number'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae549953-f48a-4da9-9b86-9c14f7c16f64",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Liste des étiquettes à prédire</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b3bec3-c292-4e58-a83e-63a58ab880c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human',\n",
       " 'Location',\n",
       " 'Reference_Code_Postal',\n",
       " 'Reference_CEDEX',\n",
       " 'Reference_CS',\n",
       " 'Function',\n",
       " 'Project',\n",
       " 'Email',\n",
       " 'Phone_Number',\n",
       " 'Social_Network',\n",
       " 'Reference_User',\n",
       " 'Organization',\n",
       " 'Url']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_annotations['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95b0a9-07e5-47b0-b7aa-2c92694c09ab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Les étiquettes sont-elles chevauchées?</div>\n",
    "\n",
    "> Oui, il semble un bruit dans l'outil d'annotation. Il suffit de surrprimer celui contenu dans la vraie étiquette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7e5f47-c02e-494a-8949-515b7f5c9e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>jdr</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: Sibyla Chandonnet\\nTechnicienne en radiologie / Health For All\\nTel : +33 0365962110 \\nsibylachandonnet@yahoo.fr\\n89 rue du Général Ailleret 62300 Lens\\nCedex 08 CS 40362\\nThe Happy Bear / happybearhealth.com\\nLinkedin : https://fr.linkedin.com/in/sibylac\n",
      "{'form': 'Technicienne', 'label': 'Function', 'begin': 19, 'end': 31}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 27, 'end': 29}\n",
      "===============\n",
      "TEXT: Dorothée Charrette\\n94 rue du Faubourg National 94320 Thiais\\nCedex 02\\n+33 01 70 92 06 69 - dorotheecharette@outlook.com\\nEscrow Papers - escrowpapers.fr\\nPoste : Technicienne en téléphonie mobile\n",
      "{'form': 'Technicienne', 'label': 'Function', 'begin': 164, 'end': 176}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 172, 'end': 174}\n",
      "===============\n",
      "TEXT: Coralie Sanschagrin\\nVendeuse en magasin - Supermarket Lists\\nsupermarketlists.fr\\nAdresse : 12 place Maurice-Charretier 94220 Charenton-sur-le-Pont\\nTél : 0171220748\\nEmail : coraliesanschagrin@gmail.com\n",
      "{'form': 'Vendeuse', 'label': 'Function', 'begin': 21, 'end': 29}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 22, 'end': 24}\n",
      "===============\n",
      "TEXT: Octave Bernier\\nVendeur en boutique - Perles et Strass\\nperlesstrass.fr\\n01.07.16.35.66\\noctavebernier@outlook.com\\nAdresse : 75019 Paris France - 2 Faubourg Saint Honoré\n",
      "{'form': 'Vendeur', 'label': 'Function', 'begin': 16, 'end': 23}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 17, 'end': 19}\n",
      "===============\n",
      "TEXT: Landry Monrency\\n79 rue Grande Fusterie 91800 Brunot\\nTél : 01.18.60.73.47\\nEmail : landrymonrency@yahoo.fr\\nVendeur en Pharmacie - Pharmaplis\\npharmaplis.com\n",
      "{'form': 'Vendeur', 'label': 'Function', 'begin': 109, 'end': 116}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 110, 'end': 112}\n",
      "===============\n",
      "TEXT: Charles Voisine\\nTél : 04.28.88.77.19\\nEmail : charlesvoisine@gmail.com\\nAdresse : 51 boulevard d'Alsace 69120 Vaulx-en-Velin\\nSerrurier - Witmark\\nwitmark.com\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 86, 'end': 95}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 94, 'end': 95}\n",
      "===============\n",
      "TEXT: Monique Saucier\\nFrance, 92170 Vanves, 16 boulevard d'Alsace\\n01 35 90 80 12\\nmoniquesaucier@gmail.com\\nAfrican CDs\\nPoste : Vendeuse en boutique\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 42, 'end': 51}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 50, 'end': 51}\n",
      "===============\n",
      "TEXT: Monique Saucier\\nFrance, 92170 Vanves, 16 boulevard d'Alsace\\n01 35 90 80 12\\nmoniquesaucier@gmail.com\\nAfrican CDs\\nPoste : Vendeuse en boutique\n",
      "{'form': 'Vendeuse', 'label': 'Function', 'begin': 125, 'end': 133}\n",
      "{'form': 'en', 'label': 'Function', 'begin': 126, 'end': 128}\n",
      "===============\n",
      "TEXT: Patrick Richard\\nGestionnaire de stocks / Laneco\\nTél : 01 09 18 31 44\\nAdresse : 78140 Vélizy-Villacoublay, France - 55 boulevard d'Alsace\\npatrichrichard@yahoo.fr\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 121, 'end': 130}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 129, 'end': 130}\n",
      "===============\n",
      "TEXT: Georges Lafontaine\\nCuisinier - Food Fair\\nAdresse : 70 boulevard d'Alsace 69200 Vénissieux\\nTél : 0419569361\\nEmail : georgeslafontaine@gmail.com\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 56, 'end': 65}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 64, 'end': 65}\n",
      "===============\n",
      "TEXT: Fabien Lejeune\\nTél : 03.82.43.44.54\\n34, boulevard d'Alsace, Verdun\\nPakla Hotel\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 42, 'end': 51}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 50, 'end': 51}\n",
      "===============\n",
      "TEXT: France Sicard\\nSicard Services\\n76 boulevard d'Alsace, Vanves\\nTél : 04 25 45 78 96\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 35, 'end': 44}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 43, 'end': 44}\n",
      "===============\n",
      "TEXT: Gaetan Desforges\\nArk Design\\n06.32.15.42.65\\nAdresse : 79 boulevard d'Alsace, Verdun\n",
      "{'form': 'boulevard', 'label': 'Location', 'begin': 59, 'end': 68}\n",
      "{'form': 'd', 'label': 'Location', 'begin': 67, 'end': 68}\n",
      "===============\n",
      "TEXT: Felipe Ramos\\nAdresse : Avinguda Soler 3-42\\nEl Juan - Colombie\\nTél : 995610494\\nGestora Rendón e Hijo\n",
      "{'form': 'Rendón', 'label': 'Organization', 'begin': 90, 'end': 96}\n",
      "{'form': 'e', 'label': 'Organization', 'begin': 91, 'end': 92}\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>jdf</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check_overlapping(data):\n",
    "    for data_row in data:\n",
    "        annotations = data_row['annotations']\n",
    "        for pre, post in zip(annotations[:-1], annotations[1:]):\n",
    "            if pre['end'] > post['begin']:\n",
    "                print('TEXT:', data_row['text'])\n",
    "                print(pre)\n",
    "                print(post)\n",
    "                print('='*15)\n",
    "\n",
    "def remove_overlapping(data):\n",
    "    for data_row in data:\n",
    "        annotations = data_row['annotations']\n",
    "        for pre, post in zip(annotations[:-1], annotations[1:]):\n",
    "            if pre['end'] >= post['end']:\n",
    "                annotations.remove(post)\n",
    "\n",
    "for split in data:\n",
    "    display(HTML('<h3>'+split+'</h3>'))\n",
    "    check_overlapping(data[split])\n",
    "\n",
    "for split in data:\n",
    "    remove_overlapping(data[split])\n",
    "    check_overlapping(data[split])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bef222-846e-453f-9cb0-6ca3733dbb71",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Est-ce qu'il existe un exemple qui manque d'annotation? Combien? Lesquels?</div>\n",
    "\n",
    "> Non, apparemment très cohérent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66c69c54-4a27-402e-b6e3-bd3d69eea537",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in data:\n",
    "    for data_row in data[split]:\n",
    "        annotations = data_row['annotations']\n",
    "        if len(annotations) == 0:\n",
    "            print(data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0aef61-f5a4-4b5b-9d3b-d6429f1203c4",
   "metadata": {},
   "source": [
    "## Tokenization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "31ea5d27-bc78-4087-ae91-d430b144c0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Faustin Chabot\\\\r\\\\nAdresse : 19 rue Descartes 94370 Sucy-en-Brie (France)\\\\r\\\\nCedex 9 CS 12468\\\\r\\\\nData Engineer / Algorithm XZ Project\\\\r\\\\nfaustinchabot@teleworm.com / Tel : +33 0134354919\\\\r\\\\nLinkedin : https://fr.linkedin.com/in/fauchab\\\\r\\\\nTeleworm France\\\\r\\\\nteleworm.france.com',\n",
       " 'Vallis Lachance\\\\r\\\\nConcepteur de publications web - Un Site, une BD\\\\r\\\\n14 rue Victor Hugo 60200 Compiègne\\\\r\\\\nCedex 12 CS 10202\\\\r\\\\nTel : 03.18.38.37.37\\\\r\\\\nEmail : vallislachance@monwax.com\\\\r\\\\nMonwax \\\\r\\\\nmonwax.com\\\\r\\\\nFacebook : https://www.facebook.com/vallislachance']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file sentencepiece.bpe.model from cache at .cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at .cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at .cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"camembert-base\",\n",
      "  \"architectures\": [\n",
      "    \"CamembertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁Faust', 'in', '▁Cha', 'bot', '\\\\', 'r', '\\\\', 'n', 'Ad', 'resse', '▁:', '▁19', '▁rue', '▁Descartes', '▁94', '370', '▁Suc', 'y', '-', 'en', '-', 'B', 'rie', '▁(', 'France', ')', '\\\\', 'r', '\\\\', 'n', 'Ce', 'dex', '▁9', '▁CS', '▁124', '68', '\\\\', 'r', '\\\\', 'n', 'D', 'ata', '▁Engine', 'er', '▁/', '▁Al', 'gor', 'ith', 'm', '▁X', 'Z', '▁Project', '\\\\', 'r', '\\\\', 'n', 'fa', 'ustin', 'cha', 'bot', '@', 'tel', 'e', 'w', 'orm', '.', 'com', '▁/', '▁Tel', '▁:', '▁+', '33', '▁01', '34', '35', '49', '19', '\\\\', 'r', '\\\\', 'n', 'L', 'ink', 'e', 'din', '▁:', '▁https', '://', 'fr', '.', 'link', 'e', 'din', '.', 'com', '/', 'in', '/', 'fa', 'uch', 'ab', '\\\\', 'r', '\\\\', 'n', 'T', 'ele', 'w', 'orm', '▁France', '\\\\', 'r', '\\\\', 'nt', 'ele', 'w', 'orm', '.', 'france', '.', 'com', '</s>']\n",
      "['<s>', '▁Val', 'lis', '▁La', 'ch', 'ance', '\\\\', 'r', '\\\\', 'n', 'Con', 'cept', 'eur', '▁de', '▁publications', '▁web', '▁-', '▁Un', '▁Site', ',', '▁une', '▁BD', '\\\\', 'r', '\\\\', 'n', '14', '▁rue', '▁Victor', '▁Hugo', '▁60', '200', '▁Compiègne', '\\\\', 'r', '\\\\', 'n', 'Ce', 'dex', '▁12', '▁CS', '▁102', '02', '\\\\', 'r', '\\\\', 'n', 'T', 'el', '▁:', '▁03', '.', '18', '.', '38', '.', '37', '.', '37', '\\\\', 'r', '\\\\', 'n', 'E', 'mail', '▁:', '▁val', 'lis', 'la', 'ch', 'ance', '@', 'mon', 'wa', 'x', '.', 'com', '\\\\', 'r', '\\\\', 'n', 'Mon', 'wa', 'x', '▁\\\\', 'r', '\\\\', 'n', 'mon', 'wa', 'x', '.', 'com', '\\\\', 'r', '\\\\', 'n', 'F', 'ace', 'book', '▁:', '▁https', '://', 'www', '.', 'facebook', '.', 'com', '/', 'val', 'lis', 'la', 'ch', 'ance', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizerFast\n",
    "\n",
    "MAX_LINE = 1000000\n",
    "CACHE_DIR = path.join('.cache', 'transformers')\n",
    "\n",
    "\n",
    "texts = [d['text'] for d in data['jdr'][:MAX_LINE]]\n",
    "print('Testing text:')\n",
    "display(texts[:2])\n",
    "\n",
    "#fast_tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\", cache_dir=CACHE_DIR, additional_special_tokens=['\\\\n', '\\\\r', 'https://'])\n",
    "fast_tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\", cache_dir=CACHE_DIR)\n",
    "\n",
    "token_encodings = fast_tokenizer(texts, return_offsets_mapping=True)\n",
    "\n",
    "token_strings = [fast_tokenizer.convert_ids_to_tokens(input_ids) for input_ids in token_encodings.input_ids]\n",
    "\n",
    "for tkstr in token_strings[:2]:\n",
    "    print(tkstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "66954b44-43bb-4a1c-844d-6742f3214015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2label = ['I-Reference_CS', 'I-Reference_User', 'B-Reference_User', 'B-Organization', 'B-Reference_CS', 'I-Human', 'I-Email', 'O', 'B-Human', 'B-Project', 'I-Social_Network', 'I-Reference_CEDEX', 'I-Url', 'B-Url', 'I-Function', 'I-Location', 'B-Email', 'B-Social_Network', 'B-Reference_CEDEX', 'B-Phone_Number', 'B-Location', 'B-Function', 'I-Phone_Number', 'I-Organization', 'I-Reference_Code_Postal', 'I-Project', 'B-Reference_Code_Postal']\n",
      "label2id = {'I-Reference_CS': 0, 'I-Reference_User': 1, 'B-Reference_User': 2, 'B-Organization': 3, 'B-Reference_CS': 4, 'I-Human': 5, 'I-Email': 6, 'O': 7, 'B-Human': 8, 'B-Project': 9, 'I-Social_Network': 10, 'I-Reference_CEDEX': 11, 'I-Url': 12, 'B-Url': 13, 'I-Function': 14, 'I-Location': 15, 'B-Email': 16, 'B-Social_Network': 17, 'B-Reference_CEDEX': 18, 'B-Phone_Number': 19, 'B-Location': 20, 'B-Function': 21, 'I-Phone_Number': 22, 'I-Organization': 23, 'I-Reference_Code_Postal': 24, 'I-Project': 25, 'B-Reference_Code_Postal': 26}\n"
     ]
    }
   ],
   "source": [
    "def mapping_label_token(token_span_batch, annotations_batch):\n",
    "    \"\"\"\n",
    "    Remap IOB tag to each token generated by tokenizer. Should provide the span (begin/end)\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = list()\n",
    "    \n",
    "    for token_span_sent, annotations in zip(token_span_batch, annotations_batch):\n",
    "        \n",
    "        annotations = annotations.copy()\n",
    "        entity = annotations.pop(0)\n",
    "        \n",
    "        last_label = 'O'\n",
    "        token_label = list()\n",
    "        \n",
    "        for token in token_span_sent:\n",
    "            \n",
    "            while entity['end'] < token['begin']: entity = annotations.pop(0)\n",
    "\n",
    "            if token['begin'] == token['end']:\n",
    "                label = 'O'    \n",
    "            elif entity['begin'] <= token['begin'] and token['end'] <= entity['end']:\n",
    "                prefix = 'B-' if last_label == 'O' or last_label[2:] != entity['label'] else 'I-'\n",
    "                label = prefix + entity['label']\n",
    "            else:\n",
    "                label = 'O'\n",
    "                \n",
    "            token_label.append(label)\n",
    "            last_label = label\n",
    "                \n",
    "        labels.append(token_label)\n",
    "        \n",
    "    return labels\n",
    "\n",
    "def tokenize_text(texts, annotations, tokenizer):\n",
    "    \n",
    "    # Tokenize text\n",
    "    token_encodings = tokenizer(texts, return_offsets_mapping=True)\n",
    "    token_encodings['tokens'] = [fast_tokenizer.convert_ids_to_tokens(input_ids) for input_ids in token_encodings.input_ids]\n",
    "    \n",
    "    # Mapping labels\n",
    "    token_span = token_encodings.offset_mapping\n",
    "    token_span_dict = [[{'begin': span[0], 'end': span[1]} for span in token_sent ] for token_sent in token_span]\n",
    "    token_encodings['ner_tags'] = mapping_label_token(token_span_dict, annotations)\n",
    "    \n",
    "    return token_encodings\n",
    "\n",
    "annotations = [d['annotations'] for d in data['jdr'][:MAX_LINE]]\n",
    "texts = [d['text'] for d in data['jdr'][:MAX_LINE]]\n",
    "tokenized = tokenize_text(texts, annotations, fast_tokenizer)\n",
    "\n",
    "all_labels = [i for l in tokenized['ner_tags'] for i in l ]\n",
    "unique_label = set(all_labels)\n",
    "id2label = list(set(all_labels))\n",
    "print('id2label =',id2label)\n",
    "label2id = {label: idx for idx, label in enumerate(id2label)}\n",
    "print('label2id =',label2id)\n",
    "\n",
    "tokenized['labels'] = [[label2id[label] for label in label_sentence] for label_sentence in tokenized['ner_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a92e95c1-8b9a-464c-9781-e36ba26944b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(tokenized))\n",
    "os.makedirs(path.join('..', '.cache'), exist_ok=True)\n",
    "df.to_csv(path.join('..','.cache', 'jdr.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d18a9ff0-3b91-47fc-a341-44dd5e3483b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>offset_mapping</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 28119, 236, 2614, 8674, 3155, 81, 3155, 25...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 5), (5, 7), (8, 11), (11, 14), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Faust, in, ▁Cha, bot, \\, r, \\, n, Ad, r...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 20, 15, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5, 1598, 4026, 61, 751, 1269, 3155, 81, 3155,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 3), (3, 6), (7, 9), (9, 11), (11,...</td>\n",
       "      <td>[&lt;s&gt;, ▁Val, lis, ▁La, ch, ance, \\, r, \\, n, Co...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, I-Huma...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 5, 7, 7, 7, 7, 21, 14, 14, 14,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5, 11904, 73, 6445, 276, 8348, 88, 3155, 81, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 4), (4, 6), (6, 8), (8, 10), (11,...</td>\n",
       "      <td>[&lt;s&gt;, ▁Arch, ai, mb, au, ▁Mass, on, \\, r, \\, n...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, I-Huma...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 5, 5, 7, 7, 7, 7, 21, 14, 14, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5, 470, 1606, 9313, 2265, 3155, 81, 3155, 255...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 4), (4, 8), (9, 12), (12, 16), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Jean, ette, ▁Fre, mont, \\, r, \\, n, 8, ...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 7, 7, 7, 7, 20, 15, 15, 15, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5, 3696, 19483, 236, 3155, 81, 3155, 255, 137...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 4), (5, 8), (8, 10), (10, 11), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Cher, ▁Baz, in, \\, r, \\, n, Mé, can, ic...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, O, O, O, O, B-F...</td>\n",
       "      <td>[7, 8, 5, 5, 7, 7, 7, 7, 21, 14, 14, 14, 14, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>[5, 4114, 61, 29807, 3155, 255, 6179, 7148, 43...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 6), (7, 9), (9, 13), (13, 14), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Claude, ▁La, ndry, \\, n, Ad, resse, ▁:,...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, O, O, O, O, O, ...</td>\n",
       "      <td>[7, 8, 5, 5, 7, 7, 7, 7, 7, 20, 15, 15, 15, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>[5, 11853, 9625, 10, 1981, 3155, 255, 3853, 30...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 9), (10, 13), (13, 14), (14, 18),...</td>\n",
       "      <td>[&lt;s&gt;, ▁Charlotte, ▁Bus, s, ière, \\, n, 59, ▁co...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 7, 7, 20, 15, 15, 15, 15, 26, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>[5, 18467, 24817, 3155, 255, 3225, 9, 3220, 9,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 6), (7, 14), (14, 15), (15, 16), ...</td>\n",
       "      <td>[&lt;s&gt;, ▁Cédric, ▁Garnier, \\, n, 04, ., 27, ., 1...</td>\n",
       "      <td>[O, B-Human, I-Human, O, O, B-Phone_Number, I-...</td>\n",
       "      <td>[7, 8, 5, 7, 7, 19, 22, 22, 22, 22, 22, 22, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>[5, 14147, 10223, 11734, 4461, 3155, 255, 3395...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 3), (3, 9), (10, 13), (13, 16), (...</td>\n",
       "      <td>[&lt;s&gt;, ▁Fla, vienne, ▁Dev, ost, \\, n, 02, ., 56...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 7, 7, 19, 22, 22, 22, 22, 22, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>[5, 19937, 487, 10340, 2872, 3155, 255, 585, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[(0, 0), (0, 6), (6, 8), (9, 12), (12, 16), (1...</td>\n",
       "      <td>[&lt;s&gt;, ▁Violet, te, ▁Fau, bert, \\, n, O, m, ni,...</td>\n",
       "      <td>[O, B-Human, I-Human, I-Human, I-Human, O, O, ...</td>\n",
       "      <td>[7, 8, 5, 5, 5, 7, 7, 3, 23, 23, 23, 23, 7, 7,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>473 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_ids  \\\n",
       "0    [5, 28119, 236, 2614, 8674, 3155, 81, 3155, 25...   \n",
       "1    [5, 1598, 4026, 61, 751, 1269, 3155, 81, 3155,...   \n",
       "2    [5, 11904, 73, 6445, 276, 8348, 88, 3155, 81, ...   \n",
       "3    [5, 470, 1606, 9313, 2265, 3155, 81, 3155, 255...   \n",
       "4    [5, 3696, 19483, 236, 3155, 81, 3155, 255, 137...   \n",
       "..                                                 ...   \n",
       "468  [5, 4114, 61, 29807, 3155, 255, 6179, 7148, 43...   \n",
       "469  [5, 11853, 9625, 10, 1981, 3155, 255, 3853, 30...   \n",
       "470  [5, 18467, 24817, 3155, 255, 3225, 9, 3220, 9,...   \n",
       "471  [5, 14147, 10223, 11734, 4461, 3155, 255, 3395...   \n",
       "472  [5, 19937, 487, 10340, 2872, 3155, 255, 585, 2...   \n",
       "\n",
       "                                        attention_mask  \\\n",
       "0    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "..                                                 ...   \n",
       "468  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "469  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "470  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "471  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "472  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                        offset_mapping  \\\n",
       "0    [(0, 0), (0, 5), (5, 7), (8, 11), (11, 14), (1...   \n",
       "1    [(0, 0), (0, 3), (3, 6), (7, 9), (9, 11), (11,...   \n",
       "2    [(0, 0), (0, 4), (4, 6), (6, 8), (8, 10), (11,...   \n",
       "3    [(0, 0), (0, 4), (4, 8), (9, 12), (12, 16), (1...   \n",
       "4    [(0, 0), (0, 4), (5, 8), (8, 10), (10, 11), (1...   \n",
       "..                                                 ...   \n",
       "468  [(0, 0), (0, 6), (7, 9), (9, 13), (13, 14), (1...   \n",
       "469  [(0, 0), (0, 9), (10, 13), (13, 14), (14, 18),...   \n",
       "470  [(0, 0), (0, 6), (7, 14), (14, 15), (15, 16), ...   \n",
       "471  [(0, 0), (0, 3), (3, 9), (10, 13), (13, 16), (...   \n",
       "472  [(0, 0), (0, 6), (6, 8), (9, 12), (12, 16), (1...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [<s>, ▁Faust, in, ▁Cha, bot, \\, r, \\, n, Ad, r...   \n",
       "1    [<s>, ▁Val, lis, ▁La, ch, ance, \\, r, \\, n, Co...   \n",
       "2    [<s>, ▁Arch, ai, mb, au, ▁Mass, on, \\, r, \\, n...   \n",
       "3    [<s>, ▁Jean, ette, ▁Fre, mont, \\, r, \\, n, 8, ...   \n",
       "4    [<s>, ▁Cher, ▁Baz, in, \\, r, \\, n, Mé, can, ic...   \n",
       "..                                                 ...   \n",
       "468  [<s>, ▁Claude, ▁La, ndry, \\, n, Ad, resse, ▁:,...   \n",
       "469  [<s>, ▁Charlotte, ▁Bus, s, ière, \\, n, 59, ▁co...   \n",
       "470  [<s>, ▁Cédric, ▁Garnier, \\, n, 04, ., 27, ., 1...   \n",
       "471  [<s>, ▁Fla, vienne, ▁Dev, ost, \\, n, 02, ., 56...   \n",
       "472  [<s>, ▁Violet, te, ▁Fau, bert, \\, n, O, m, ni,...   \n",
       "\n",
       "                                              ner_tags  \\\n",
       "0    [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "1    [O, B-Human, I-Human, I-Human, I-Human, I-Huma...   \n",
       "2    [O, B-Human, I-Human, I-Human, I-Human, I-Huma...   \n",
       "3    [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "4    [O, B-Human, I-Human, I-Human, O, O, O, O, B-F...   \n",
       "..                                                 ...   \n",
       "468  [O, B-Human, I-Human, I-Human, O, O, O, O, O, ...   \n",
       "469  [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "470  [O, B-Human, I-Human, O, O, B-Phone_Number, I-...   \n",
       "471  [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "472  [O, B-Human, I-Human, I-Human, I-Human, O, O, ...   \n",
       "\n",
       "                                                labels  \n",
       "0    [7, 8, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 20, 15, 1...  \n",
       "1    [7, 8, 5, 5, 5, 5, 7, 7, 7, 7, 21, 14, 14, 14,...  \n",
       "2    [7, 8, 5, 5, 5, 5, 5, 7, 7, 7, 7, 21, 14, 14, ...  \n",
       "3    [7, 8, 5, 5, 5, 7, 7, 7, 7, 20, 15, 15, 15, 15...  \n",
       "4    [7, 8, 5, 5, 7, 7, 7, 7, 21, 14, 14, 14, 14, 7...  \n",
       "..                                                 ...  \n",
       "468  [7, 8, 5, 5, 7, 7, 7, 7, 7, 20, 15, 15, 15, 15...  \n",
       "469  [7, 8, 5, 5, 5, 7, 7, 20, 15, 15, 15, 15, 26, ...  \n",
       "470  [7, 8, 5, 7, 7, 19, 22, 22, 22, 22, 22, 22, 22...  \n",
       "471  [7, 8, 5, 5, 5, 7, 7, 19, 22, 22, 22, 22, 22, ...  \n",
       "472  [7, 8, 5, 5, 5, 7, 7, 3, 23, 23, 23, 23, 7, 7,...  \n",
       "\n",
       "[473 rows x 6 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "81fafc63-f55f-4927-9a4e-f68dc4e990c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at .cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"camembert-base\",\n",
      "  \"architectures\": [\n",
      "    \"CamembertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": [\n",
      "    \"I-Reference_CS\",\n",
      "    \"I-Reference_User\",\n",
      "    \"B-Reference_User\",\n",
      "    \"B-Organization\",\n",
      "    \"B-Reference_CS\",\n",
      "    \"I-Human\",\n",
      "    \"I-Email\",\n",
      "    \"O\",\n",
      "    \"B-Human\",\n",
      "    \"B-Project\",\n",
      "    \"I-Social_Network\",\n",
      "    \"I-Reference_CEDEX\",\n",
      "    \"I-Url\",\n",
      "    \"B-Url\",\n",
      "    \"I-Function\",\n",
      "    \"I-Location\",\n",
      "    \"B-Email\",\n",
      "    \"B-Social_Network\",\n",
      "    \"B-Reference_CEDEX\",\n",
      "    \"B-Phone_Number\",\n",
      "    \"B-Location\",\n",
      "    \"B-Function\",\n",
      "    \"I-Phone_Number\",\n",
      "    \"I-Organization\",\n",
      "    \"I-Reference_Code_Postal\",\n",
      "    \"I-Project\",\n",
      "    \"B-Reference_Code_Postal\"\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-Email\": 16,\n",
      "    \"B-Function\": 21,\n",
      "    \"B-Human\": 8,\n",
      "    \"B-Location\": 20,\n",
      "    \"B-Organization\": 3,\n",
      "    \"B-Phone_Number\": 19,\n",
      "    \"B-Project\": 9,\n",
      "    \"B-Reference_CEDEX\": 18,\n",
      "    \"B-Reference_CS\": 4,\n",
      "    \"B-Reference_Code_Postal\": 26,\n",
      "    \"B-Reference_User\": 2,\n",
      "    \"B-Social_Network\": 17,\n",
      "    \"B-Url\": 13,\n",
      "    \"I-Email\": 6,\n",
      "    \"I-Function\": 14,\n",
      "    \"I-Human\": 5,\n",
      "    \"I-Location\": 15,\n",
      "    \"I-Organization\": 23,\n",
      "    \"I-Phone_Number\": 22,\n",
      "    \"I-Project\": 25,\n",
      "    \"I-Reference_CEDEX\": 11,\n",
      "    \"I-Reference_CS\": 0,\n",
      "    \"I-Reference_Code_Postal\": 24,\n",
      "    \"I-Reference_User\": 1,\n",
      "    \"I-Social_Network\": 10,\n",
      "    \"I-Url\": 12,\n",
      "    \"O\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at .cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/pytorch_model.bin\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 473\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 900\n",
      "  Number of trainable parameters = 110052123\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='391' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/900 1:10:57 < 1:32:50, 0.09 it/s, Epoch 13/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Email F1</th>\n",
       "      <th>Function F1</th>\n",
       "      <th>Human F1</th>\n",
       "      <th>Location F1</th>\n",
       "      <th>Organization F1</th>\n",
       "      <th>Phone Number F1</th>\n",
       "      <th>Project F1</th>\n",
       "      <th>Reference Cedex F1</th>\n",
       "      <th>Reference Cs F1</th>\n",
       "      <th>Reference Code Postal F1</th>\n",
       "      <th>Reference User F1</th>\n",
       "      <th>Social Network F1</th>\n",
       "      <th>Url F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.084700</td>\n",
       "      <td>2.008339</td>\n",
       "      <td>0.332868</td>\n",
       "      <td>0.322084</td>\n",
       "      <td>0.327387</td>\n",
       "      <td>0.726547</td>\n",
       "      <td>0.673861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.369293</td>\n",
       "      <td>0.090382</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.627900</td>\n",
       "      <td>1.541823</td>\n",
       "      <td>0.822362</td>\n",
       "      <td>0.849892</td>\n",
       "      <td>0.835900</td>\n",
       "      <td>0.891661</td>\n",
       "      <td>0.659280</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.898117</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.304600</td>\n",
       "      <td>1.260516</td>\n",
       "      <td>0.942572</td>\n",
       "      <td>0.943844</td>\n",
       "      <td>0.943208</td>\n",
       "      <td>0.957257</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.964147</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.982236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.529801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.971554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.293100</td>\n",
       "      <td>1.069506</td>\n",
       "      <td>0.954422</td>\n",
       "      <td>0.949784</td>\n",
       "      <td>0.952097</td>\n",
       "      <td>0.973457</td>\n",
       "      <td>0.975207</td>\n",
       "      <td>0.940754</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.971401</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.992608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.970200</td>\n",
       "      <td>0.927045</td>\n",
       "      <td>0.954989</td>\n",
       "      <td>0.950864</td>\n",
       "      <td>0.952922</td>\n",
       "      <td>0.978515</td>\n",
       "      <td>0.976821</td>\n",
       "      <td>0.929329</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.973340</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.993658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.995595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.967200</td>\n",
       "      <td>0.809946</td>\n",
       "      <td>0.949490</td>\n",
       "      <td>0.954104</td>\n",
       "      <td>0.951791</td>\n",
       "      <td>0.984751</td>\n",
       "      <td>0.989933</td>\n",
       "      <td>0.939286</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.973812</td>\n",
       "      <td>0.941520</td>\n",
       "      <td>0.993658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998145</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.784400</td>\n",
       "      <td>0.709403</td>\n",
       "      <td>0.947467</td>\n",
       "      <td>0.954374</td>\n",
       "      <td>0.950908</td>\n",
       "      <td>0.985740</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.916376</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.973812</td>\n",
       "      <td>0.949853</td>\n",
       "      <td>0.993658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.624450</td>\n",
       "      <td>0.949007</td>\n",
       "      <td>0.954644</td>\n",
       "      <td>0.951817</td>\n",
       "      <td>0.985816</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.937611</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.973812</td>\n",
       "      <td>0.941634</td>\n",
       "      <td>0.993658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.547984</td>\n",
       "      <td>0.958634</td>\n",
       "      <td>0.969762</td>\n",
       "      <td>0.964166</td>\n",
       "      <td>0.987983</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.906897</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.973812</td>\n",
       "      <td>0.954635</td>\n",
       "      <td>0.993658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.583000</td>\n",
       "      <td>0.484598</td>\n",
       "      <td>0.958466</td>\n",
       "      <td>0.971922</td>\n",
       "      <td>0.965147</td>\n",
       "      <td>0.991558</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.922807</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.974284</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.993658</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>0.434881</td>\n",
       "      <td>0.969981</td>\n",
       "      <td>0.977052</td>\n",
       "      <td>0.973504</td>\n",
       "      <td>0.991710</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.947748</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.974284</td>\n",
       "      <td>0.976791</td>\n",
       "      <td>0.993658</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>0.394084</td>\n",
       "      <td>0.972930</td>\n",
       "      <td>0.980022</td>\n",
       "      <td>0.976463</td>\n",
       "      <td>0.991938</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.947935</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.974284</td>\n",
       "      <td>0.989796</td>\n",
       "      <td>0.993658</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.995595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>0.359494</td>\n",
       "      <td>0.973212</td>\n",
       "      <td>0.980832</td>\n",
       "      <td>0.977007</td>\n",
       "      <td>0.993840</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.939502</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.974284</td>\n",
       "      <td>0.988810</td>\n",
       "      <td>0.993658</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.993377</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.995595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-30\n",
      "Configuration saved in ./results/checkpoint-30/config.json\n",
      "Model weights saved in ./results/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-30/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-60\n",
      "Configuration saved in ./results/checkpoint-60/config.json\n",
      "Model weights saved in ./results/checkpoint-60/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-60/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-90\n",
      "Configuration saved in ./results/checkpoint-90/config.json\n",
      "Model weights saved in ./results/checkpoint-90/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-90/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-90/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-120\n",
      "Configuration saved in ./results/checkpoint-120/config.json\n",
      "Model weights saved in ./results/checkpoint-120/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-120/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-120/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-150\n",
      "Configuration saved in ./results/checkpoint-150/config.json\n",
      "Model weights saved in ./results/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-150/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-180\n",
      "Configuration saved in ./results/checkpoint-180/config.json\n",
      "Model weights saved in ./results/checkpoint-180/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-180/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-180/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-210\n",
      "Configuration saved in ./results/checkpoint-210/config.json\n",
      "Model weights saved in ./results/checkpoint-210/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-210/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-210/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-240\n",
      "Configuration saved in ./results/checkpoint-240/config.json\n",
      "Model weights saved in ./results/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-240/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-270\n",
      "Configuration saved in ./results/checkpoint-270/config.json\n",
      "Model weights saved in ./results/checkpoint-270/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-270/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-270/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-300\n",
      "Configuration saved in ./results/checkpoint-300/config.json\n",
      "Model weights saved in ./results/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-300/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-330\n",
      "Configuration saved in ./results/checkpoint-330/config.json\n",
      "Model weights saved in ./results/checkpoint-330/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-330/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-330/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-360\n",
      "Configuration saved in ./results/checkpoint-360/config.json\n",
      "Model weights saved in ./results/checkpoint-360/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-360/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-360/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 473\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-390\n",
      "Configuration saved in ./results/checkpoint-390/config.json\n",
      "Model weights saved in ./results/checkpoint-390/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-390/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-390/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:325] . unexpected pos 335875776 vs 335875664",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/torch/serialization.py:423\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 423\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/torch/serialization.py:650\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    649\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 650\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:450] . PytorchStreamWriter failed writing file data/126: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [86], line 78\u001b[0m\n\u001b[1;32m     57\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     58\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     59\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,  \u001b[38;5;66;03m# batch size per device during training\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     68\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     69\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     70\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 78\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/transformers/trainer.py:1841\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1841\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1845\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/transformers/trainer.py:2093\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2090\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2094\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/transformers/trainer.py:2184\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2181\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, SCALER_NAME))\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n\u001b[1;32m   2183\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 2184\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2185\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m caught_warnings:\n\u001b[1;32m   2186\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, SCHEDULER_NAME))\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/torch/serialization.py:424\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m--> 424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n",
      "File \u001b[0;32m~/venv/textmine/lib/python3.8/site-packages/torch/serialization.py:290\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:325] . unexpected pos 335875776 vs 335875664"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForTokenClassification\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextMineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.data = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self): raise IndexError  # meet the end of dataset\n",
    "        sample = self.data.loc[idx].to_dict()\n",
    "        #for k, v in sample.items():\n",
    "        #   print(k, '=', len(v))\n",
    "        return {'input_ids': sample['input_ids'], 'attention_mask': sample['attention_mask'],'labels': sample['labels']}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "dataset = TextMineDataset(df)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "        if(k not in flattened_results.keys()):\n",
    "            flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"camembert-base\", num_labels=len(id2label), id2label=id2label, label2id=label2id, cache_dir=CACHE_DIR)\n",
    "data_collator = DataCollatorForTokenClassification(fast_tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    num_train_epochs=30,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=1\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=fast_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
