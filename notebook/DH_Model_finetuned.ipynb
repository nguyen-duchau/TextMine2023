{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d51f7a-eef1-4ca1-87c6-70c406202ba3",
   "metadata": {},
   "source": [
    "# Camembert model fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "950c58eb-dfc6-46cb-baab-57f45081e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "sys.path.append(\"./../src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d804a6e-2008-4847-a196-20da4556e7ba",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dce6ccf0-8a81-402e-9c35-a21d27cdf02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = path.join('..', 'dataset')\n",
    "CACHE_DIR = path.join('..', '.cache')\n",
    "\n",
    "# load jdf and jdr\n",
    "df_jdf = pd.read_parquet(path.join(CACHE_DIR, 'jdf.parquet'))\n",
    "df_jdr = pd.read_parquet(path.join(CACHE_DIR, 'jdr.parquet'))\n",
    "\n",
    "# fusion into full data\n",
    "full_data = pd.concat([df_jdf, df_jdr], ignore_index=True)\n",
    "for col in full_data.columns:\n",
    "    if isinstance(full_data.loc[0, col], np.ndarray):\n",
    "        full_data[col] = full_data[col].apply(lambda x: x.tolist())\n",
    "full_data.to_parquet(path.join(CACHE_DIR, 'full.parquet'))\n",
    "\n",
    "# split and generate dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "train, val = train_test_split(full_data, test_size=.15)\n",
    "train.to_parquet(path.join(CACHE_DIR, 'train.parquet'), index=False)\n",
    "val.to_parquet(path.join(CACHE_DIR, 'val.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "87da88f3-2b4d-4fbf-829d-8d3cbbc535db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file sentencepiece.bpe.model from cache at ../.cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at ../.cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at ../.cache/transformers/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"camembert-base\",\n",
      "  \"architectures\": [\n",
      "    \"CamembertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cache data from ../.cache/jdr.parquet\n",
      "Generate new label2idx\n",
      "Load cache data from ../.cache/jdf.parquet\n",
      "Generate new label2idx\n",
      "Load cache data from ../.cache/train.parquet\n",
      "Generate new label2idx\n",
      "Load cache data from ../.cache/val.parquet\n",
      "Generate new label2idx\n"
     ]
    }
   ],
   "source": [
    "from data.textmine import TextMineDataset\n",
    "from transformers import CamembertTokenizerFast, TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForTokenClassification, EarlyStoppingCallback\n",
    "from datasets import load_metric\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\", cache_dir=path.join(CACHE_DIR, 'transformers'))\n",
    "jdr = TextMineDataset('jdr', tokenizer=tokenizer, data_path=DATA_PATH, cache=CACHE_DIR)\n",
    "jdf = TextMineDataset('jdf', tokenizer=tokenizer, data_path=DATA_PATH, cache=CACHE_DIR)\n",
    "trainset = TextMineDataset('train', tokenizer=tokenizer, data_path=DATA_PATH, cache=CACHE_DIR)\n",
    "valset = TextMineDataset('val', tokenizer=tokenizer, data_path=DATA_PATH, cache=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2b18e83a-6491-490c-8af8-0ab58f69dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "        if(k not in flattened_results.keys()):\n",
    "            flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "            flattened_results[k+\"_recall\"]=results[k][\"recall\"]\n",
    "            flattened_results[k+\"_precision\"]=results[k][\"precision\"]\n",
    "\n",
    "    return flattened_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93b1a626-e93d-4487-94d0-ac3daa5da68c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at ../.cache/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"camembert-base\",\n",
      "  \"architectures\": [\n",
      "    \"CamembertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-Email\",\n",
      "    \"2\": \"I-Email\",\n",
      "    \"3\": \"B-Function\",\n",
      "    \"4\": \"I-Function\",\n",
      "    \"5\": \"B-Human\",\n",
      "    \"6\": \"I-Human\",\n",
      "    \"7\": \"B-Location\",\n",
      "    \"8\": \"I-Location\",\n",
      "    \"9\": \"B-Organization\",\n",
      "    \"10\": \"I-Organization\",\n",
      "    \"11\": \"B-Project\",\n",
      "    \"12\": \"B-Phone_Number\",\n",
      "    \"13\": \"I-Phone_Number\",\n",
      "    \"14\": \"I-Project\",\n",
      "    \"15\": \"B-Reference_CEDEX\",\n",
      "    \"16\": \"B-Reference_User\",\n",
      "    \"17\": \"B-Reference_CS\",\n",
      "    \"18\": \"B-Reference_Code_Postal\",\n",
      "    \"19\": \"I-Reference_CS\",\n",
      "    \"20\": \"I-Reference_User\",\n",
      "    \"21\": \"I-Reference_Code_Postal\",\n",
      "    \"22\": \"I-Reference_CEDEX\",\n",
      "    \"23\": \"B-Social_Network\",\n",
      "    \"24\": \"I-Social_Network\",\n",
      "    \"25\": \"B-Url\",\n",
      "    \"26\": \"I-Url\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-Email\": 1,\n",
      "    \"B-Function\": 3,\n",
      "    \"B-Human\": 5,\n",
      "    \"B-Location\": 7,\n",
      "    \"B-Organization\": 9,\n",
      "    \"B-Phone_Number\": 12,\n",
      "    \"B-Project\": 11,\n",
      "    \"B-Reference_CEDEX\": 15,\n",
      "    \"B-Reference_CS\": 17,\n",
      "    \"B-Reference_Code_Postal\": 18,\n",
      "    \"B-Reference_User\": 16,\n",
      "    \"B-Social_Network\": 23,\n",
      "    \"B-Url\": 25,\n",
      "    \"I-Email\": 2,\n",
      "    \"I-Function\": 4,\n",
      "    \"I-Human\": 6,\n",
      "    \"I-Location\": 8,\n",
      "    \"I-Organization\": 10,\n",
      "    \"I-Phone_Number\": 13,\n",
      "    \"I-Project\": 14,\n",
      "    \"I-Reference_CEDEX\": 22,\n",
      "    \"I-Reference_CS\": 19,\n",
      "    \"I-Reference_Code_Postal\": 21,\n",
      "    \"I-Reference_User\": 20,\n",
      "    \"I-Social_Network\": 24,\n",
      "    \"I-Url\": 26,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at ../.cache/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/pytorch_model.bin\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForTokenClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {idx: label for idx, label in enumerate(trainset.id2label)}\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"camembert-base\", num_labels=len(trainset.id2label), id2label=id2label, label2id=trainset.label2id, cache_dir=CACHE_DIR)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eaf69114-1322-4be5-96ad-84df9ef852b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    num_train_epochs=30,\n",
    "    output_dir=path.join('..', '.cache', 'results', 'final'),\n",
    "    logging_dir=path.join('..', '.cache', 'logs', 'final'),\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='tensorboard'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7385ee0a-b094-421d-858e-ac9bbd0ca344",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_labels = [l for label_array in trainset.data['labels'] for l in label_array]\n",
    "label_frequency = dict()\n",
    "\n",
    "for label in flat_labels:\n",
    "    label_frequency[label] = label_frequency.get(label, 0) + 1\n",
    "\n",
    "idx_labels = list(label_frequency.keys())\n",
    "\n",
    "label_weight = [0] * len(label_frequency.keys())\n",
    "for idx, f in label_frequency.items():\n",
    "    label_weight[idx] = 1/f\n",
    "sum_weight = sum(label_weight)\n",
    "label_weight = [w/sum_weight for w in label_weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "653153cf-946f-4617-ba5a-ce317bc12796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch \n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \n",
    "    def __init__(self, label_weight: dict, **kwargs):\n",
    "        self.label_weight = label_weight\n",
    "        return super(CustomTrainer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        \n",
    "        # compute custom loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(self.label_weight))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "trainer = CustomTrainer(\n",
    "    label_weight=label_weight,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=valset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "619e7f47-3325-4c0a-8301-f02e190c037b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 827\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 780\n",
      "  Number of trainable parameters = 110052123\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='364' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [364/780 1:39:52 < 1:54:46, 0.06 it/s, Epoch 14/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Email F1</th>\n",
       "      <th>Email Recall</th>\n",
       "      <th>Email Precision</th>\n",
       "      <th>Function F1</th>\n",
       "      <th>Function Recall</th>\n",
       "      <th>Function Precision</th>\n",
       "      <th>Human F1</th>\n",
       "      <th>Human Recall</th>\n",
       "      <th>Human Precision</th>\n",
       "      <th>Location F1</th>\n",
       "      <th>Location Recall</th>\n",
       "      <th>Location Precision</th>\n",
       "      <th>Organization F1</th>\n",
       "      <th>Organization Recall</th>\n",
       "      <th>Organization Precision</th>\n",
       "      <th>Phone Number F1</th>\n",
       "      <th>Phone Number Recall</th>\n",
       "      <th>Phone Number Precision</th>\n",
       "      <th>Project F1</th>\n",
       "      <th>Project Recall</th>\n",
       "      <th>Project Precision</th>\n",
       "      <th>Reference Cedex F1</th>\n",
       "      <th>Reference Cedex Recall</th>\n",
       "      <th>Reference Cedex Precision</th>\n",
       "      <th>Reference Cs F1</th>\n",
       "      <th>Reference Cs Recall</th>\n",
       "      <th>Reference Cs Precision</th>\n",
       "      <th>Reference Code Postal F1</th>\n",
       "      <th>Reference Code Postal Recall</th>\n",
       "      <th>Reference Code Postal Precision</th>\n",
       "      <th>Reference User F1</th>\n",
       "      <th>Reference User Recall</th>\n",
       "      <th>Reference User Precision</th>\n",
       "      <th>Social Network F1</th>\n",
       "      <th>Social Network Recall</th>\n",
       "      <th>Social Network Precision</th>\n",
       "      <th>Url F1</th>\n",
       "      <th>Url Recall</th>\n",
       "      <th>Url Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.670200</td>\n",
       "      <td>2.492454</td>\n",
       "      <td>0.873171</td>\n",
       "      <td>0.776910</td>\n",
       "      <td>0.822232</td>\n",
       "      <td>0.942541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.973154</td>\n",
       "      <td>0.973154</td>\n",
       "      <td>0.973154</td>\n",
       "      <td>0.880795</td>\n",
       "      <td>0.923611</td>\n",
       "      <td>0.841772</td>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950495</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.462700</td>\n",
       "      <td>2.398289</td>\n",
       "      <td>0.897817</td>\n",
       "      <td>0.785590</td>\n",
       "      <td>0.837963</td>\n",
       "      <td>0.946867</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.963211</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.953528</td>\n",
       "      <td>0.961806</td>\n",
       "      <td>0.945392</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.826772</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.288300</td>\n",
       "      <td>2.347303</td>\n",
       "      <td>0.911178</td>\n",
       "      <td>0.792535</td>\n",
       "      <td>0.847725</td>\n",
       "      <td>0.951367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.963211</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.957118</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.945763</td>\n",
       "      <td>0.852830</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.862595</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.192400</td>\n",
       "      <td>2.314426</td>\n",
       "      <td>0.911178</td>\n",
       "      <td>0.792535</td>\n",
       "      <td>0.847725</td>\n",
       "      <td>0.951367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.960549</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.834586</td>\n",
       "      <td>0.828358</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.046600</td>\n",
       "      <td>2.300604</td>\n",
       "      <td>0.918675</td>\n",
       "      <td>0.794271</td>\n",
       "      <td>0.851955</td>\n",
       "      <td>0.951886</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.995800</td>\n",
       "      <td>2.286223</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.853556</td>\n",
       "      <td>0.952060</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.898600</td>\n",
       "      <td>2.277956</td>\n",
       "      <td>0.919563</td>\n",
       "      <td>0.803819</td>\n",
       "      <td>0.857805</td>\n",
       "      <td>0.952752</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.963211</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.970940</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.956229</td>\n",
       "      <td>0.907749</td>\n",
       "      <td>0.917910</td>\n",
       "      <td>0.897810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.823800</td>\n",
       "      <td>2.266903</td>\n",
       "      <td>0.922542</td>\n",
       "      <td>0.806424</td>\n",
       "      <td>0.860584</td>\n",
       "      <td>0.955175</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.970940</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.956229</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.864700</td>\n",
       "      <td>2.261973</td>\n",
       "      <td>0.925224</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.861253</td>\n",
       "      <td>0.954483</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.981132</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.969492</td>\n",
       "      <td>0.904412</td>\n",
       "      <td>0.917910</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>2.269711</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.802951</td>\n",
       "      <td>0.859665</td>\n",
       "      <td>0.953098</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.963211</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.979452</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.966216</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>2.253667</td>\n",
       "      <td>0.927073</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.862053</td>\n",
       "      <td>0.954829</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.976109</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.923664</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.945312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.662400</td>\n",
       "      <td>2.271082</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.858337</td>\n",
       "      <td>0.954656</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.976027</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.962838</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.930769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.621100</td>\n",
       "      <td>2.275973</td>\n",
       "      <td>0.923307</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>0.859926</td>\n",
       "      <td>0.955002</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.979452</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.966216</td>\n",
       "      <td>0.913858</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.917293</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.590600</td>\n",
       "      <td>2.273916</td>\n",
       "      <td>0.933868</td>\n",
       "      <td>0.809028</td>\n",
       "      <td>0.866977</td>\n",
       "      <td>0.956040</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.979452</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.966216</td>\n",
       "      <td>0.951311</td>\n",
       "      <td>0.947761</td>\n",
       "      <td>0.954887</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-26\n",
      "Configuration saved in ../.cache/results/final/checkpoint-26/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-26/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-26/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-26/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-52\n",
      "Configuration saved in ../.cache/results/final/checkpoint-52/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-52/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-52/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-52/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-78\n",
      "Configuration saved in ../.cache/results/final/checkpoint-78/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-78/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-78/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-78/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/final/checkpoint-26] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-104\n",
      "Configuration saved in ../.cache/results/final/checkpoint-104/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-104/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-104/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-104/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/final/checkpoint-52] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-130\n",
      "Configuration saved in ../.cache/results/final/checkpoint-130/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-130/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-130/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-130/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/final/checkpoint-78] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-156\n",
      "Configuration saved in ../.cache/results/final/checkpoint-156/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-156/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-156/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-156/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/final/checkpoint-104] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-182\n",
      "Configuration saved in ../.cache/results/final/checkpoint-182/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-182/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-182/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-182/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/final/checkpoint-130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-208\n",
      "Configuration saved in ../.cache/results/final/checkpoint-208/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/final/checkpoint-156] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-234\n",
      "Configuration saved in ../.cache/results/final/checkpoint-234/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-234/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-234/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-234/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/final/checkpoint-182] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-260\n",
      "Configuration saved in ../.cache/results/final/checkpoint-260/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-260/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-260/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-260/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/final/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-286\n",
      "Configuration saved in ../.cache/results/final/checkpoint-286/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-286/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-286/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-286/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/final/checkpoint-234] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-312\n",
      "Configuration saved in ../.cache/results/final/checkpoint-312/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-312/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-312/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-312/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/final/checkpoint-260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-338\n",
      "Configuration saved in ../.cache/results/final/checkpoint-338/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-338/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-338/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-338/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/final/checkpoint-312] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../.cache/results/final/checkpoint-364\n",
      "Configuration saved in ../.cache/results/final/checkpoint-364/config.json\n",
      "Model weights saved in ../.cache/results/final/checkpoint-364/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/final/checkpoint-364/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/final/checkpoint-364/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/final/checkpoint-338] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../.cache/results/final/checkpoint-286 (score: 2.253667116165161).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=364, training_loss=1.0484168075925702, metrics={'train_runtime': 6014.2827, 'train_samples_per_second': 4.125, 'train_steps_per_second': 0.13, 'total_flos': 406589507285670.0, 'train_loss': 1.0484168075925702, 'epoch': 14.0})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de1231-fb43-445a-8d28-238f67ee3bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0fd64-bdf5-4037-a1df-cf592a557088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at ../.cache/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"camembert-base\",\n",
      "  \"architectures\": [\n",
      "    \"CamembertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-Email\",\n",
      "    \"2\": \"I-Email\",\n",
      "    \"3\": \"B-Function\",\n",
      "    \"4\": \"I-Function\",\n",
      "    \"5\": \"B-Human\",\n",
      "    \"6\": \"I-Human\",\n",
      "    \"7\": \"B-Location\",\n",
      "    \"8\": \"I-Location\",\n",
      "    \"9\": \"B-Organization\",\n",
      "    \"10\": \"I-Organization\",\n",
      "    \"11\": \"B-Project\",\n",
      "    \"12\": \"B-Phone_Number\",\n",
      "    \"13\": \"I-Phone_Number\",\n",
      "    \"14\": \"I-Project\",\n",
      "    \"15\": \"B-Reference_CEDEX\",\n",
      "    \"16\": \"B-Reference_User\",\n",
      "    \"17\": \"B-Reference_CS\",\n",
      "    \"18\": \"B-Reference_Code_Postal\",\n",
      "    \"19\": \"I-Reference_CS\",\n",
      "    \"20\": \"I-Reference_User\",\n",
      "    \"21\": \"I-Reference_Code_Postal\",\n",
      "    \"22\": \"I-Reference_CEDEX\",\n",
      "    \"23\": \"B-Social_Network\",\n",
      "    \"24\": \"I-Social_Network\",\n",
      "    \"25\": \"B-Url\",\n",
      "    \"26\": \"I-Url\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-Email\": 1,\n",
      "    \"B-Function\": 3,\n",
      "    \"B-Human\": 5,\n",
      "    \"B-Location\": 7,\n",
      "    \"B-Organization\": 9,\n",
      "    \"B-Phone_Number\": 12,\n",
      "    \"B-Project\": 11,\n",
      "    \"B-Reference_CEDEX\": 15,\n",
      "    \"B-Reference_CS\": 17,\n",
      "    \"B-Reference_Code_Postal\": 18,\n",
      "    \"B-Reference_User\": 16,\n",
      "    \"B-Social_Network\": 23,\n",
      "    \"B-Url\": 25,\n",
      "    \"I-Email\": 2,\n",
      "    \"I-Function\": 4,\n",
      "    \"I-Human\": 6,\n",
      "    \"I-Location\": 8,\n",
      "    \"I-Organization\": 10,\n",
      "    \"I-Phone_Number\": 13,\n",
      "    \"I-Project\": 14,\n",
      "    \"I-Reference_CEDEX\": 22,\n",
      "    \"I-Reference_CS\": 19,\n",
      "    \"I-Reference_Code_Postal\": 21,\n",
      "    \"I-Reference_User\": 20,\n",
      "    \"I-Social_Network\": 24,\n",
      "    \"I-Url\": 26,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at ../.cache/models--camembert-base/snapshots/3f452b6e5a89b0e6c828c9bba2642bc577086eae/pytorch_model.bin\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForTokenClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 827\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 520\n",
      "  Number of trainable parameters = 110052123\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/520 30:34 < 1:57:26, 0.06 it/s, Epoch 4.15/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Email F1</th>\n",
       "      <th>Email Recall</th>\n",
       "      <th>Email Precision</th>\n",
       "      <th>Function F1</th>\n",
       "      <th>Function Recall</th>\n",
       "      <th>Function Precision</th>\n",
       "      <th>Human F1</th>\n",
       "      <th>Human Recall</th>\n",
       "      <th>Human Precision</th>\n",
       "      <th>Location F1</th>\n",
       "      <th>Location Recall</th>\n",
       "      <th>Location Precision</th>\n",
       "      <th>Organization F1</th>\n",
       "      <th>Organization Recall</th>\n",
       "      <th>Organization Precision</th>\n",
       "      <th>Phone Number F1</th>\n",
       "      <th>Phone Number Recall</th>\n",
       "      <th>Phone Number Precision</th>\n",
       "      <th>Project F1</th>\n",
       "      <th>Project Recall</th>\n",
       "      <th>Project Precision</th>\n",
       "      <th>Reference Cedex F1</th>\n",
       "      <th>Reference Cedex Recall</th>\n",
       "      <th>Reference Cedex Precision</th>\n",
       "      <th>Reference Cs F1</th>\n",
       "      <th>Reference Cs Recall</th>\n",
       "      <th>Reference Cs Precision</th>\n",
       "      <th>Reference Code Postal F1</th>\n",
       "      <th>Reference Code Postal Recall</th>\n",
       "      <th>Reference Code Postal Precision</th>\n",
       "      <th>Reference User F1</th>\n",
       "      <th>Reference User Recall</th>\n",
       "      <th>Reference User Precision</th>\n",
       "      <th>Social Network F1</th>\n",
       "      <th>Social Network Recall</th>\n",
       "      <th>Social Network Precision</th>\n",
       "      <th>Url F1</th>\n",
       "      <th>Url Recall</th>\n",
       "      <th>Url Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.230200</td>\n",
       "      <td>2.183944</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>0.712184</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.907216</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.801262</td>\n",
       "      <td>0.852349</td>\n",
       "      <td>0.755952</td>\n",
       "      <td>0.101053</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.128342</td>\n",
       "      <td>0.111628</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.595918</td>\n",
       "      <td>0.598361</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.760900</td>\n",
       "      <td>1.698980</td>\n",
       "      <td>0.838677</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>0.778605</td>\n",
       "      <td>0.869505</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.898223</td>\n",
       "      <td>0.965278</td>\n",
       "      <td>0.839879</td>\n",
       "      <td>0.724832</td>\n",
       "      <td>0.805970</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.434400</td>\n",
       "      <td>1.425219</td>\n",
       "      <td>0.869439</td>\n",
       "      <td>0.780382</td>\n",
       "      <td>0.822507</td>\n",
       "      <td>0.928868</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.903021</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.832845</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.243500</td>\n",
       "      <td>1.259483</td>\n",
       "      <td>0.914915</td>\n",
       "      <td>0.793403</td>\n",
       "      <td>0.849837</td>\n",
       "      <td>0.940810</td>\n",
       "      <td>0.974619</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.967521</td>\n",
       "      <td>0.982639</td>\n",
       "      <td>0.952862</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.906475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../.cache/results/train_val_split/checkpoint-26\n",
      "Configuration saved in ../.cache/results/train_val_split/checkpoint-26/config.json\n",
      "Model weights saved in ../.cache/results/train_val_split/checkpoint-26/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_val_split/checkpoint-26/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_val_split/checkpoint-26/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_val_split/checkpoint-52] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../.cache/results/train_val_split/checkpoint-52\n",
      "Configuration saved in ../.cache/results/train_val_split/checkpoint-52/config.json\n",
      "Model weights saved in ../.cache/results/train_val_split/checkpoint-52/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_val_split/checkpoint-52/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_val_split/checkpoint-52/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_val_split/checkpoint-26] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../.cache/results/train_val_split/checkpoint-78\n",
      "Configuration saved in ../.cache/results/train_val_split/checkpoint-78/config.json\n",
      "Model weights saved in ../.cache/results/train_val_split/checkpoint-78/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_val_split/checkpoint-78/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_val_split/checkpoint-78/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_val_split/checkpoint-52] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 146\n",
      "  Batch size = 32\n",
      "/Users/dunguyen/venv/textmine/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../.cache/results/train_val_split/checkpoint-104\n",
      "Configuration saved in ../.cache/results/train_val_split/checkpoint-104/config.json\n",
      "Model weights saved in ../.cache/results/train_val_split/checkpoint-104/pytorch_model.bin\n",
      "tokenizer config file saved in ../.cache/results/train_val_split/checkpoint-104/tokenizer_config.json\n",
      "Special tokens file saved in ../.cache/results/train_val_split/checkpoint-104/special_tokens_map.json\n",
      "Deleting older checkpoint [../.cache/results/train_val_split/checkpoint-78] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "model2 = AutoModelForTokenClassification.from_pretrained(\"camembert-base\", num_labels=len(trainset.id2label), id2label=id2label, label2id=trainset.label2id, cache_dir=CACHE_DIR)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    num_train_epochs=20,\n",
    "    output_dir=path.join('..', '.cache', 'results', 'train_val_split'),\n",
    "    logging_dir=path.join('..', '.cache', 'logs', 'train_val_split'),\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='tensorboard'\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=valset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf7a92-9f2e-4000-9c7b-422df908d662",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04cc2bc-db2c-4f05-a3e1-9c696da5e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "JDA_PATH = path.join(DATA_PATH, 'JDA.json')\n",
    "\n",
    "with open(JDA_PATH, 'r') as f:\n",
    "    data_jda = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d1997-cb30-49cc-a9b2-29c2b0d69b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List):\n",
    "    return max(set(List), key = List.count)\n",
    "\n",
    "for idx_sent, sentence in enumerate(data_jda):\n",
    "    tokens = tokenizer(sentence['text'], return_offsets_mapping=True, return_tensors='pt')\n",
    "    offsets = tokens.pop('offset_mapping')\n",
    "    offsets = offsets.squeeze()\n",
    "    results = model(**tokens)\n",
    "    label_bert = results.logits.squeeze().argmax(dim=1)\n",
    "    label_bert = torch.cat([offsets, label_bert.unsqueeze(dim=1)], dim=1)\n",
    "    idx_predict = 0\n",
    "    \n",
    "    annotations = list()\n",
    "    \n",
    "    for entity in sentence['annotations']:\n",
    "        \n",
    "        predictions = list()\n",
    "        \n",
    "        while label_bert[idx_predict][0] < entity['begin'] or label_bert[idx_predict][1] < entity['end']:\n",
    "            idx_predict += 1\n",
    "        \n",
    "        while label_bert[idx_predix][0] < entity['end']:\n",
    "            entity_label = int(label_bert[idx_predict, 2])\n",
    "            entity_label = jdf.id2label[entity_label]\n",
    "            entity_label = entity_label if len(entity_label) == 1 else entity_label[2:]\n",
    "            predictions.append(entity_label)\n",
    "        \n",
    "        \n",
    "        entity['label_bert'] = most_frequent(predictions)\n",
    "        annotations.append(entity)\n",
    "        \n",
    "    data_jda[idx_sent]['annotations'] = annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed541613-df20-4be4-b583-a1c56ec764cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join(DATA_PATH, 'jda_bert.json'), 'w') as f:\n",
    "    json.dump(data_jda, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0952b0-a84d-4b18-b48c-6f6ac30ec5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
